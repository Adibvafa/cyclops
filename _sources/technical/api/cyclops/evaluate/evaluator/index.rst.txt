:py:mod:`cyclops.evaluate.evaluator`
====================================

.. py:module:: cyclops.evaluate.evaluator

.. autoapi-nested-parse::

   Evaluator class.

   ..
       !! processed by numpydoc !!


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   cyclops.evaluate.evaluator.Evaluator




.. py:class:: Evaluator(models: Union[cyclops.models.wrappers.WrappedModel, Sequence[cyclops.models.wrappers.WrappedModel], Dict[str, cyclops.models.wrappers.WrappedModel]], data: NamedTuple, metrics: Union[cyclops.evaluate.metrics.metric.Metric, Sequence[cyclops.evaluate.metrics.metric.Metric], Dict[str, cyclops.evaluate.metrics.metric.Metric], cyclops.evaluate.metrics.metric.MetricCollection])

   
   Evaluate one or more models on a dataset.

   .. attribute:: models

      Model(s) to evaluate.

      :type: Dict[str, WrappedModel]

   .. attribute:: data

      Dataset to evaluate the model(s) on.

      :type: NamedTuple

   .. attribute:: metrics

      Metric(s) to use for evaluation.

      :type: MetricCollection

   .. attribute:: warning:: This class is experimental and will change in the future.















   ..
       !! processed by numpydoc !!
   .. py:method:: _prepare_models(model: Union[cyclops.models.wrappers.WrappedModel, Sequence[cyclops.models.wrappers.WrappedModel], Dict[str, cyclops.models.wrappers.WrappedModel]]) -> Dict[str, cyclops.models.wrappers.WrappedModel]

      
      Prepare the model(s) for evaluation.

      :param model: Model(s) to evaluate.
      :type model: Union[WrappedModel, Sequence[WrappedModel], Dict[str, WrappedModel]]

      :returns: **model** -- Model(s) to evaluate.
      :rtype: Dict[str, WrappedModel]















      ..
          !! processed by numpydoc !!

   .. py:method:: _prepare_metrics(metrics: Union[cyclops.evaluate.metrics.metric.Metric, Sequence[cyclops.evaluate.metrics.metric.Metric], Dict[str, cyclops.evaluate.metrics.metric.Metric], cyclops.evaluate.metrics.metric.MetricCollection]) -> cyclops.evaluate.metrics.metric.MetricCollection

      
      Prepare the metric(s) for evaluation.

      A MetricCollection object is returned, which is a dict-like object that
      can be used to compute the metric(s) on a set of predictions.

      :param metrics: Metric(s) to use for evaluation.
      :type metrics: Union[Metric, Sequence[Metric], Dict[str, Metric], MetricCollection]

      :returns: Metric(s) to use for evaluation.
      :rtype: MetricCollection















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> Dict[str, Dict[str, float]]

      
      Evaluate the model(s) on the dataset.

      :returns: **scores** -- Dictionary containing the evaluation results.
      :rtype: Dict[str, Dict[str, float]]















      ..
          !! processed by numpydoc !!


