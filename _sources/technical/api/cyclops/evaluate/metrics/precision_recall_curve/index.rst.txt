:py:mod:`cyclops.evaluate.metrics.precision_recall_curve`
=========================================================

.. py:module:: cyclops.evaluate.metrics.precision_recall_curve

.. autoapi-nested-parse::

   Classes for computing precision-recall curves.

   ..
       !! processed by numpydoc !!


Module Contents
---------------

.. py:class:: BinaryPrecisionRecallCurve(thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1)



   
   Compute precision-recall curve for binary input.

   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list or numpy.ndarray, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param pos_label: The label of the positive class.
   :type pos_label: int

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryPrecisionRecallCurve
   >>> target = [0, 1, 0, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> metric = BinaryPrecisionRecallCurve(thresholds=3)
   >>> metric(target, preds)
   (array([0.5, 1. , 0. ]), array([1. , 0.5, 0. ]), array([0. , 0.5, 1. ]))
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [1, 1, 0, 0]]
   >>> preds = [[0.1, 0.4, 0.35, 0.8], [0.6, 0.3, 0.1, 0.7]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([0.5       , 0.66666667, 0.        ]),
   array([1. , 0.5, 0. ]),
   array([0. , 0.5, 1. ]))















   ..
       !! processed by numpydoc !!
   .. py:method:: update_state(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike) -> None

      
      Update the state of the metric.

      The state is either a list of targets and predictions (if ``thresholds`` is
      ``None``) or a confusion matrix.















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]

      
      Compute the precision-recall curve from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassPrecisionRecallCurve(num_classes: int, thresholds: Union[int, List[float], numpy.ndarray] = None)



   
   Compute the precision-recall curve for multiclass problems.

   :param num_classes: The number of classes in the dataset.
   :type num_classes: int
   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassPrecisionRecallCurve
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0.],
   ...          [0.5, 0.3, 0.2], [0.2, 0.5, 0.3]]
   >>> metric = MulticlassPrecisionRecallCurve(num_classes=3, thresholds=3)
   >>> metric(target, preds)
   (array([[0.5       , 0.        , 0.        , 1.        ],
           [0.25      , 0.33333333, 0.        , 1.        ],
           [0.25      , 0.        , 0.        , 1.        ]]),
   array([[1., 0., 0., 0.],
           [1., 1., 0., 0.],
           [1., 0., 0., 0.]]),
   array([0. , 0.5, 1. ]))
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [1, 2, 0, 1]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3], [0.05, 0.95, 0.], [0.5, 0.3, 0.2], [0.2, 0.5, 0.3]],
   ...     [[0.3, 0.2, 0.5], [0.1, 0.7, 0.2], [0.6, 0.1, 0.3], [0.1, 0.8, 0.1]],
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0.375, 0.5  , 0.   , 1.   ],
           [0.375, 0.4  , 0.   , 1.   ],
           [0.25 , 0.   , 0.   , 1.   ]]),
       array([[1.        , 0.33333333, 0.        , 0.        ],
           [1.        , 0.66666667, 0.        , 0.        ],
           [1.        , 0.        , 0.        , 0.        ]]),
       array([0. , 0.5, 1. ]))















   ..
       !! processed by numpydoc !!
   .. py:method:: update_state(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike) -> None

      
      Update the state of the metric.

      The state is either a list of targets and predictions (if ``thresholds`` is
      ``None``) or a confusion matrix.















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> Union[Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray], Tuple[List[numpy.ndarray], List[numpy.ndarray], List[numpy.ndarray]]]

      
      Compute the precision-recall curve from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelPrecisionRecallCurve(num_labels: int, thresholds: Union[int, List[float], numpy.ndarray] = None)



   
   Check and format the multilabel precision-recall curve input/data.

   :param num_labels: The number of labels in the dataset.
   :type num_labels: int
   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int, list of floats or numpy.ndarray of floats, default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelPrecisionRecallCurve
   >>> target = [[0, 1], [1, 0]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> metric = MultilabelPrecisionRecallCurve(num_labels=2, thresholds=3)
   >>> metric(target, preds)
   (array([[0.5, 1. , 0. , 1. ],
           [0.5, 1. , 0. , 1. ]]),
   array([[1., 1., 0., 0.],
           [1., 1., 0., 0.]]),
   array([0. , 0.5, 1. ]))
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 0]], [[1, 0], [0, 1]]]
   >>> preds = [[[0.1, 0.9], [0.8, 0.2]], [[0.2, 0.8], [0.7, 0.3]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0.5, 0.5, 0. , 1. ],
           [0.5, 0.5, 0. , 1. ]]),
   array([[1. , 0.5, 0. , 0. ],
           [1. , 0.5, 0. , 0. ]]),
   array([0. , 0.5, 1. ]))















   ..
       !! processed by numpydoc !!
   .. py:method:: update_state(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike) -> None

      
      Update the state of the metric.

      The state is either a list of targets and predictions (if ``thresholds`` is
      ``None``) or a confusion matrix.















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]

      
      Compute the precision-recall curve from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: PrecisionRecallCurve



   
   Compute the precision-recall curve for different classification tasks.

   :param task: The task for which the precision-recall curve is computed.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param thresholds: Thresholds used for computing the precision and recall scores. If int,
                      then the number of thresholds to use. If list or array, then the
                      thresholds to use. If None, then the thresholds are automatically
                      determined by the sunique values in ``preds``
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: The number of classes in the dataset. Required if ``task`` is
                       ``"multiclass"``.
   :type num_classes: int, optional
   :param num_labels: The number of labels in the dataset. Required if ``task`` is
                      ``"multilabel"``.
   :type num_labels: int, optional

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import PrecisionRecallCurve
   >>> target = [1, 1, 1, 0]
   >>> preds = [0.6, 0.2, 0.3, 0.8]
   >>> metric = PrecisionRecallCurve(task="binary", thresholds=None)
   >>> metric(target, preds)
   (array([0.75      , 0.66666667, 0.5       , 0.        , 1.        ]),
   array([1.        , 0.66666667, 0.33333333, 0.        , 0.        ]),
   array([0.2, 0.3, 0.6, 0.8]))
   >>> metric.reset_state()
   >>> target = [[1, 0, 1, 1], [0, 0, 0, 1]]
   >>> preds = [[0.5, 0.4, 0.1, 0.3], [0.9, 0.6, 0.45, 0.8]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([0.5       , 0.42857143, 0.33333333, 0.4       , 0.5       ,
       0.33333333, 0.5       , 0.        , 1.        ]),
   array([1.  , 0.75, 0.5 , 0.5 , 0.5 , 0.25, 0.25, 0.  , 0.  ]),
   array([0.1 , 0.3 , 0.4 , 0.45, 0.5 , 0.6 , 0.8 , 0.9 ]))

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import PrecisionRecallCurve
   >>> target = [0, 1, 2, 2]
   >>> preds = [[0.05, 0.95, 0], [0.1, 0.8, 0.1],
   ...         [0.2, 0.2, 0.6], [0.2, 0.2, 0.6]]
   >>> metric = PrecisionRecallCurve(task="multiclass", num_classes=3,
   ...     thresholds=3)
   >>> metric(target, preds)
   (array([[0.25, 0.  , 0.  , 1.  ],
           [0.25, 0.5 , 0.  , 1.  ],
           [0.5 , 1.  , 0.  , 1.  ]]),
   array([[1., 0., 0., 0.],
           [1., 1., 0., 0.],
           [1., 1., 0., 0.]]),
   array([0. , 0.5, 1. ]))
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 2], [1, 2, 0, 1]]
   >>> preds = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1],
   ...         [0.2, 0.2, 0.6], [0.2, 0.2, 0.6]],
   ...         [[0.05, 0.95, 0], [0.1, 0.8, 0.1],
   ...         [0.2, 0.2, 0.6], [0.2, 0.2, 0.6]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0.25 , 0.   , 0.   , 1.   ],
           [0.375, 0.5  , 0.   , 1.   ],
           [0.375, 0.5  , 0.   , 1.   ]]),
   array([[1.        , 0.        , 0.        , 0.        ],
           [1.        , 0.66666667, 0.        , 0.        ],
           [1.        , 0.66666667, 0.        , 0.        ]]),
   array([0. , 0.5, 1. ]))

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import PrecisionRecallCurve
   >>> target = [[0, 1], [1, 0]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> metric = PrecisionRecallCurve(task="multilabel", num_labels=2,
   ...     thresholds=3)
   >>> metric(target, preds)
   (array([[0.5, 1. , 0. , 1. ],
           [0.5, 1. , 0. , 1. ]]),
   array([[1., 1., 0., 0.],
           [1., 1., 0., 0.]]),
   array([0. , 0.5, 1. ]))
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 0]], [[1, 0], [0, 1]]]
   >>> preds = [[[0.1, 0.9], [0.8, 0.2]],
   ...         [[0.1, 0.9], [0.8, 0.2]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0.5, 0.5, 0. , 1. ],
           [0.5, 0.5, 0. , 1. ]]),
   array([[1. , 0.5, 0. , 0. ],
           [1. , 0.5, 0. , 0. ]]),
   array([0. , 0.5, 1. ]))















   ..
       !! processed by numpydoc !!

