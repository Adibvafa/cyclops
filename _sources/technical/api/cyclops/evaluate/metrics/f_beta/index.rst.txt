:py:mod:`cyclops.evaluate.metrics.f_beta`
=========================================

.. py:module:: cyclops.evaluate.metrics.f_beta

.. autoapi-nested-parse::

   Classes for computing the F-beta score.

   ..
       !! processed by numpydoc !!


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.f_beta.BinaryFbetaScore
   cyclops.evaluate.metrics.f_beta.MulticlassFbetaScore
   cyclops.evaluate.metrics.f_beta.MultilabelFbetaScore
   cyclops.evaluate.metrics.f_beta.FbetaScore
   cyclops.evaluate.metrics.f_beta.BinaryF1Score
   cyclops.evaluate.metrics.f_beta.MulticlassF1Score
   cyclops.evaluate.metrics.f_beta.MultilabelF1Score
   cyclops.evaluate.metrics.f_beta.F1Score




.. py:class:: BinaryFbetaScore(beta: float, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.BinaryStatScores`

   
   Compute the F-beta score for binary classification tasks.

   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param pos_label: The positive class label. One of [0, 1].
   :type pos_label: int, default=1
   :param threshold: Threshold value for converting probabilities and logits to binary.
                     Logits will be converted to probabilities using the sigmoid function.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there are no true positives or true negatives.
                         If set to ``warn``, this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryFbetaScore
   >>> target = [0, 1, 1, 0]
   >>> preds = [0, 1, 0, 0]
   >>> metric = BinaryFbetaScore(beta=0.5)
   >>> metric(target, preds)
   0.8333333333333334
   >>> metric.reset_state()
   >>> target = [[1, 0, 1, 0], [1, 0, 0, 1]]
   >>> preds = [[0.2, 0.8, 0.3, 0.4], [0.6, 0.3, 0.1, 0.5]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.625















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the metric from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassFbetaScore(beta: float, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MulticlassStatScores`

   
   Compute the F-beta score for multiclass classification tasks.

   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param num_classes: The number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassFbetaScore
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> metric = MulticlassFbetaScore(beta=0.5, num_classes=3)
   >>> metric(target, preds)
   array([1., 0., 0.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.90909091, 0.        , 0.        ])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the metric from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelFbetaScore(beta: float, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MultilabelStatScores`

   
   Compute the F-beta score for multilabel classification tasks.

   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class if predicitions are logits
                     or probability scores. Logits will be converted to probabilities using
                     the sigmoid function.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each label. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelFbetaScore
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> metric = MultilabelFbetaScore(beta=0.5, num_labels=2)
   >>> metric(target, preds)
   array([1.        , 0.83333333])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [[[0, 1], [1, 0]], [[1, 1], [1, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1.        , 0.90909091])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the metric from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: FbetaScore

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the F-beta score for different types of classification tasks.

   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import FbetaScore
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.8, 0.4, 0.3]
   >>> metric = FbetaScore(beta=0.5, task="binary")
   >>> metric(target, preds)
   0.8333333333333334
   >>> metric.reset_state()
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.9090909090909091

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import FbetaScore
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.1, 0.8, 0.1], [0.1, 0.1, 0.8], [0.1, 0.1, 0.8], [0.8, 0.1, 0.1]]
   >>> metric = FbetaScore(beta=0.5, task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([0.83333333, 0.        , 0.55555556])
   >>> metric.reset_state()
   >>> target = [[0, 1, 0], [0, 0, 1]]
   >>> preds = [[[0.1, 0.8, 0.1], [0.1, 0.1, 0.8], [0.8, 0.1, 0.1]],
   ...          [[0.1, 0.1, 0.8], [0.8, 0.1, 0.1], [0.1, 0.8, 0.1]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.83333333, 0.5       , 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import FbetaScore
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> metric = FbetaScore(beta=0.5, task="multilabel", num_labels=2)
   >>> metric(target, preds)
   array([1.        , 0.83333333])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [[[0, 1], [1, 0]], [[1, 1], [1, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1.        , 0.90909091])















   ..
       !! processed by numpydoc !!

.. py:class:: BinaryF1Score(pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`BinaryFbetaScore`

   
   Compute the F1 score for binary classification tasks.

   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold value for binarizing predictions in form of logits or
                     probability scores.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryF1Score
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.8, 0.4, 0.3]
   >>> metric = BinaryF1Score()
   >>> metric(target, preds)
   0.6666666666666666
   >>> metric.reset_state()
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.8















   ..
       !! processed by numpydoc !!

.. py:class:: MulticlassF1Score(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`MulticlassFbetaScore`

   
   Compute the F1 score for multiclass classification tasks.

   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each class. Otherwise, use one of
                   the following options to compute the average score:

                   - ``micro``: Calculate metric globally.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class
                       imbalance. It can result in an F-score that is not between
                       precision and recall.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassF1Score
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.95, 0.05, 0]]
   >>> metric = MulticlassF1Score(num_classes=3)
   >>> metric(target, preds)
   array([0.66666667, 0.5       , 0.        ])
   >>> metric.reset_state()
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[[0.1, 0.9, 0], [0.05, 0.95, 0]],
   ...         [[0.1, 0.8, 0.1], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.        , 0.85714286, 0.        ])















   ..
       !! processed by numpydoc !!

.. py:class:: MultilabelF1Score(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`MultilabelFbetaScore`

   
   Compute the F1 score for multilabel classification tasks.

   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each label. Otherwise, use one of
                   the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false positives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelF1Score
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.2]]
   >>> metric = MultilabelF1Score(num_labels=3)
   >>> metric(target, preds)
   array([0., 1., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7, 0.2], [0.2, 0.8, 0.3]],
   ...     [[0.5, 0.9, 0.0], [0.3, 0.4, 0.2]],
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0. , 0.8, 0. ])















   ..
       !! processed by numpydoc !!

.. py:class:: F1Score

   Bases: :py:obj:`FbetaScore`

   
   Compute the F1 score for different types of classification tasks.

   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import F1Score
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.8, 0.4, 0.3]
   >>> metric = F1Score(task="binary")
   >>> metric(target, preds)
   0.6666666666666666
   >>> metric.reset_state()
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.8

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import F1Score
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.95, 0.05, 0]]
   >>> metric = F1Score(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([0.66666667, 0.5       , 0.        ])
   >>> metric.reset_state()
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[[0.1, 0.9, 0], [0.05, 0.95, 0]],
   ...         [[0.1, 0.8, 0.1], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.        , 0.85714286, 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import F1Score
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.2]]
   >>> metric = F1Score(task="multilabel", num_labels=3)
   >>> metric(target, preds)
   array([0., 1., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7, 0.2], [0.2, 0.8, 0.3]],
   ...     [[0.5, 0.9, 0.0], [0.3, 0.4, 0.2]],
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0. , 0.8, 0. ])















   ..
       !! processed by numpydoc !!

