:py:mod:`cyclops.evaluate.metrics.accuracy`
===========================================

.. py:module:: cyclops.evaluate.metrics.accuracy

.. autoapi-nested-parse::

   Classes for computing accuracy metrics.

   ..
       !! processed by numpydoc !!


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.accuracy.BinaryAccuracy
   cyclops.evaluate.metrics.accuracy.MulticlassAccuracy
   cyclops.evaluate.metrics.accuracy.MultilabelAccuracy
   cyclops.evaluate.metrics.accuracy.Accuracy




.. py:class:: BinaryAccuracy(threshold: float = 0.5, pos_label: int = 1, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.BinaryStatScores`

   
   Compute accuracy score for binary classification tasks.

   :param pos_label: The label of the positive class. Can be 0 or 1.
   :type pos_label: int, default=1
   :param threshold: The threshold value for converting probability or logit scores to
                     binary. A sigmoid function is first applied to logits to convert them
                     to probabilities.
   :type threshold: float, default=0.5
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryAccuracy
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = BinaryAccuracy()
   >>> metric(target, preds)
   0.75
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [1, 0, 1, 0]]
   >>> preds = [[0, 1, 1, 1], [1, 0, 1, 0]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.875















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the accuracy score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassAccuracy(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MulticlassStatScores`

   
   Compute the accuracy score for multiclass classification problems.

   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: Number of highest probability predictions or logits to consider when
                 computing the accuracy score.
   :type top_k: int, default=None
   :param average: If not None, this determines the type of averaging performed on the data:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each class, and find their unweighted
                       mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metrics for each class, and find their average,
                       weighted by support (the number of true instances for each class).
                       This alters ``macro`` to account for class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassAccuracy
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 0, 2, 2, 1]
   >>> metric = MulticlassAccuracy(num_classes=3)
   >>> metric(target, preds)
   array([1.        , 0.        , 0.66666667])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2], [2, 1, 0]]
   >>> preds = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.2, 0.6, 0.2]],
   ...          [[0.1, 0.8, 0.1], [0.05, 0.95, 0], [0.2, 0.6, 0.2]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0., 1., 0.])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the accuracy score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelAccuracy(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MultilabelStatScores`

   
   Compute the accuracy score for multilabel-indicator targets.

   :param num_labels: Number of labels in the multilabel classification task.
   :type num_labels: int
   :param threshold: Threshold value for binarizing the output of the classifier.
   :type threshold: float, default=0.5
   :param top_k: The number of highest probability or logit predictions considered
                 to find the correct label. Only works when ``preds`` contains
                 probabilities/logits.
   :type top_k: int, optional, default=None
   :param average: If None, return the accuracy score per label, otherwise this determines
                   the type of averaging performed on the data:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their
                       average, weighted by support (the number of true instances for
                       each label).
   :type average: Literal['micro', 'macro', 'weighted', None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal['warn', 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelAccuracy
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0, 1, 0], [1, 0, 1]]
   >>> metric = MultilabelAccuracy(num_labels=3)
   >>> metric(target, preds)
   array([1., 1., 0.])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 1], [1, 0, 0]], [[1, 0, 0], [0, 1, 1]]]
   >>> preds = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1]],
   ...          [[0.1, 0.8, 0.1], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.5, 0.5, 0.5])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the accuracy score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: Accuracy

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute accuracy score for different classification tasks.

   :param task: The type of task for the input data. One of 'binary', 'multiclass'
                or 'multilabel'.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the recall score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import Accuracy
   >>> target = [0, 0, 1, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = Accuracy(task="binary")
   >>> metric(target, preds)
   0.75
   >>> metric.reset_state()
   >>> target = [[0, 0, 1, 1], [1, 1, 0, 0]]
   >>> preds = [[0.05, 0.95, 0, 0], [0.1, 0.8, 0.1, 0]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.5

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import Accuracy
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 0, 2, 2, 1]
   >>> metric = Accuracy(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([1.        , 0.        , 0.66666667])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2], [2, 1, 0]]
   >>> preds = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.2, 0.6, 0.2]],
   ...          [[0.1, 0.8, 0.1], [0.05, 0.95, 0], [0.2, 0.6, 0.2]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0., 1., 0.])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import Accuracy
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0, 1, 0], [1, 0, 1]]
   >>> metric = Accuracy(task="multilabel", num_labels=3)
   >>> metric(target, preds)
   array([1., 1., 0.])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 1], [1, 0, 0]], [[1, 0, 0], [0, 1, 1]]]
   >>> preds = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1]],
   ...          [[0.1, 0.8, 0.1], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.5, 0.5, 0.5])















   ..
       !! processed by numpydoc !!

