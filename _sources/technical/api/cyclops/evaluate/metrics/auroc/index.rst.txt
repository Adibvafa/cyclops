:py:mod:`cyclops.evaluate.metrics.auroc`
========================================

.. py:module:: cyclops.evaluate.metrics.auroc

.. autoapi-nested-parse::

   Classes for computing area under the ROC curve.

   ..
       !! processed by numpydoc !!


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.auroc.BinaryAUROC
   cyclops.evaluate.metrics.auroc.MulticlassAUROC
   cyclops.evaluate.metrics.auroc.MultilabelAUROC
   cyclops.evaluate.metrics.auroc.AUROC




.. py:class:: BinaryAUROC(max_fpr: float = None, thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1)

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall_curve.BinaryPrecisionRecallCurve`

   
   Compute the area under the ROC curve for binary classification tasks.

   :param max_fpr: The maximum value of the false positive rate. If not None, then
                   the partial AUCROC in the range [0, max_fpr] is returned.
   :type max_fpr: float, default=None
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryAUROC
   >>> target = [0, 0, 1, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> metric = BinaryAUROC()
   >>> metric(target, preds)
   0.75
   >>> metric.reset_state()
   >>> target = [[0, 1, 0], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.7, 0.2, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.6111111111111112















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the area under the ROC curve from the state variables.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassAUROC(num_classes: int, thresholds: Union[int, List[float], numpy.ndarray] = None, average: Literal[macro, weighted] = None)

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall_curve.MulticlassPrecisionRecallCurve`

   
   Compute the area under the ROC curve for multiclass classification tasks.

   :param num_classes: Number of classes.
   :type num_classes: int
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None
   :param average: If ``None``, then the scores for each class are returned. Otherwise,
                   this determines the type of averaging performed on the scores. One of:

                   - `macro`: Calculate metrics for each class, and find their unweighted
                       mean. This does not take class imbalance into account.
                   - `weighted`: Calculate metrics for each class, and find their average,
                       weighted by support (the number of true instances for each class).
   :type average: Literal["macro", "weighted"], default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassAUROC
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.89, 0.06],
   ...         [0.05, 0.01, 0.94], [0.9, 0.05, 0.05]]
   >>> metric = MulticlassAUROC(num_classes=3)
   >>> metric(target, preds)
   array([1., 1., 1.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 0], [1, 0, 1]]
   >>> preds = [[[0.1, 0.9, 0.0], [0.7, 0.2, 0.1], [0.2, 0.3, 0.5]],
   ...         [[0.1, 0.1, 0.8], [0.7, 0.2, 0.1], [0.2, 0.3, 0.5]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.5       , 0.22222222, 0.        ])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> Union[float, numpy.ndarray]

      
      Compute the area under the ROC curve from the state variables.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelAUROC(num_labels: int, thresholds: Union[int, List[float], numpy.ndarray] = None, average: Literal[micro, macro, weighted] = None)

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall_curve.MultilabelPrecisionRecallCurve`

   
   Compute the area under the ROC curve for multilabel classification tasks.

   :param num_labels: The number of labels in the multilabel classification task.
   :type num_labels: int
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None
   :param average: If ``None``, then the scores for each label are returned. Otherwise,
                   this determines the type of averaging performed on the scores. One of:

                   - `micro`: Calculate metrics globally.
                   - `macro`: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - `weighted``: Calculate metrics for each label, and find their average,
                       weighted by support (the number of true instances for each label).
   :type average: Literal["micro", "macro", "weighted"], default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelAUROC
   >>> target = [[0, 1], [1, 1], [1, 0]]
   >>> preds = [[0.9, 0.05], [0.05, 0.89], [0.05, 0.01]]
   >>> metric = MultilabelAUROC(num_labels=2)
   >>> metric(target, preds)
   array([1., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 0]], [[1, 1], [1, 0]]]
   >>> preds = [[[0.9, 0.05], [0.05, 0.89]], [[0.05, 0.89], [0.05, 0.01]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1.   , 0.625])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> Union[float, numpy.ndarray]

      
      Compute the area under the ROC curve from the state variables.
















      ..
          !! processed by numpydoc !!


.. py:class:: AUROC

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the AUROC curve for different types of classification tasks.

   :param task: Task type. One of ``binary``, ``multiclass``, ``multilabel``.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param max_fpr: The maximum value of the false positive rate. If not None, a partial AUC
                   in the range [0, max_fpr] is returned. Only used for binary classification.
   :type max_fpr: float, default=None
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param num_classes: Number of classes. This parameter is required for the ``multiclass`` task.
   :type num_classes: int, default=None
   :param num_labels: Number of labels. This parameter is required for the ``multilabel`` task.
   :type num_labels: int, default=None
   :param average: If not None, apply the method to compute the average area under the
                   ROC curve. Only applicable for the ``multiclass`` and ``multilabel``
                   tasks. One of:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their average,
                       weighted by support (accounting for label imbalance).
   :type average: Literal["micro", "macro", "weighted"], default=None

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import BinaryAUROC
   >>> target = [0, 0, 1, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> metric = BinaryAUROC()
   >>> metric(target, preds)
   0.75
   >>> metric.reset_state()
   >>> target = [[0, 1, 0], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.7, 0.2, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.6111111111111112

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import MulticlassAUROC
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.89, 0.06],
   ...         [0.05, 0.01, 0.94], [0.9, 0.05, 0.05]]
   >>> metric = MulticlassAUROC(num_classes=3)
   >>> metric(target, preds)
   array([1., 1., 1.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 0], [1, 0, 1]]
   >>> preds = [[[0.1, 0.9, 0.0], [0.7, 0.2, 0.1], [0.2, 0.3, 0.5]],
   ...         [[0.1, 0.1, 0.8], [0.7, 0.2, 0.1], [0.2, 0.3, 0.5]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.5       , 0.22222222, 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import MultilabelAUROC
   >>> target = [[0, 1], [1, 1], [1, 0]]
   >>> preds = [[0.9, 0.05], [0.05, 0.89], [0.05, 0.01]]
   >>> metric = MultilabelAUROC(num_labels=2)
   >>> metric(target, preds)
   array([1., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 0]], [[1, 1], [1, 0]]]
   >>> preds = [[[0.9, 0.05], [0.05, 0.89]], [[0.05, 0.89], [0.05, 0.01]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1.   , 0.625])















   ..
       !! processed by numpydoc !!

