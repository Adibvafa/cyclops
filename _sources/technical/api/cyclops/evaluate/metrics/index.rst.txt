:py:mod:`cyclops.evaluate.metrics`
==================================

.. py:module:: cyclops.evaluate.metrics

.. autoapi-nested-parse::

   
   Evaluation metrics package.
















   ..
       !! processed by numpydoc !!


Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   functional/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   accuracy/index.rst
   auroc/index.rst
   f_beta/index.rst
   metric/index.rst
   precision_recall/index.rst
   precision_recall_curve/index.rst
   roc/index.rst
   sensitivity/index.rst
   specificity/index.rst
   stat_scores/index.rst
   utils/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.Accuracy
   cyclops.evaluate.metrics.BinaryAccuracy
   cyclops.evaluate.metrics.MulticlassAccuracy
   cyclops.evaluate.metrics.MultilabelAccuracy
   cyclops.evaluate.metrics.AUROC
   cyclops.evaluate.metrics.BinaryAUROC
   cyclops.evaluate.metrics.MulticlassAUROC
   cyclops.evaluate.metrics.MultilabelAUROC
   cyclops.evaluate.metrics.BinaryF1Score
   cyclops.evaluate.metrics.BinaryFbetaScore
   cyclops.evaluate.metrics.F1Score
   cyclops.evaluate.metrics.FbetaScore
   cyclops.evaluate.metrics.MulticlassF1Score
   cyclops.evaluate.metrics.MulticlassFbetaScore
   cyclops.evaluate.metrics.MultilabelF1Score
   cyclops.evaluate.metrics.MultilabelFbetaScore
   cyclops.evaluate.metrics.MetricCollection
   cyclops.evaluate.metrics.BinaryPrecision
   cyclops.evaluate.metrics.BinaryRecall
   cyclops.evaluate.metrics.MulticlassPrecision
   cyclops.evaluate.metrics.MulticlassRecall
   cyclops.evaluate.metrics.MultilabelPrecision
   cyclops.evaluate.metrics.MultilabelRecall
   cyclops.evaluate.metrics.Precision
   cyclops.evaluate.metrics.Recall
   cyclops.evaluate.metrics.BinaryPrecisionRecallCurve
   cyclops.evaluate.metrics.MulticlassPrecisionRecallCurve
   cyclops.evaluate.metrics.MultilabelPrecisionRecallCurve
   cyclops.evaluate.metrics.PrecisionRecallCurve
   cyclops.evaluate.metrics.BinaryROCCurve
   cyclops.evaluate.metrics.MulticlassROCCurve
   cyclops.evaluate.metrics.MultilabelROCCurve
   cyclops.evaluate.metrics.ROCCurve
   cyclops.evaluate.metrics.BinarySensitivity
   cyclops.evaluate.metrics.MulticlassSensitivity
   cyclops.evaluate.metrics.MultilabelSensitivity
   cyclops.evaluate.metrics.Sensitivity
   cyclops.evaluate.metrics.BinarySpecificity
   cyclops.evaluate.metrics.MulticlassSpecificity
   cyclops.evaluate.metrics.MultilabelSpecificity
   cyclops.evaluate.metrics.Specificity
   cyclops.evaluate.metrics.BinaryStatScores
   cyclops.evaluate.metrics.MulticlassStatScores
   cyclops.evaluate.metrics.MultilabelStatScores
   cyclops.evaluate.metrics.StatScores



Functions
~~~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.create_metric



.. py:class:: Accuracy

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute accuracy score for different classification tasks.

   :param task: The type of task for the input data. One of 'binary', 'multiclass'
                or 'multilabel'.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the recall score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import Accuracy
   >>> target = [0, 0, 1, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = Accuracy(task="binary")
   >>> metric(target, preds)
   0.75
   >>> metric.reset_state()
   >>> target = [[0, 0, 1, 1], [1, 1, 0, 0]]
   >>> preds = [[0.05, 0.95, 0, 0], [0.1, 0.8, 0.1, 0]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.5

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import Accuracy
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 0, 2, 2, 1]
   >>> metric = Accuracy(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([1.        , 0.        , 0.66666667])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2], [2, 1, 0]]
   >>> preds = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.2, 0.6, 0.2]],
   ...          [[0.1, 0.8, 0.1], [0.05, 0.95, 0], [0.2, 0.6, 0.2]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0., 1., 0.])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import Accuracy
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0, 1, 0], [1, 0, 1]]
   >>> metric = Accuracy(task="multilabel", num_labels=3)
   >>> metric(target, preds)
   array([1., 1., 0.])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 1], [1, 0, 0]], [[1, 0, 0], [0, 1, 1]]]
   >>> preds = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1]],
   ...          [[0.1, 0.8, 0.1], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.5, 0.5, 0.5])















   ..
       !! processed by numpydoc !!

.. py:class:: BinaryAccuracy(threshold: float = 0.5, pos_label: int = 1, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.BinaryStatScores`

   
   Compute accuracy score for binary classification tasks.

   :param pos_label: The label of the positive class. Can be 0 or 1.
   :type pos_label: int, default=1
   :param threshold: The threshold value for converting probability or logit scores to
                     binary. A sigmoid function is first applied to logits to convert them
                     to probabilities.
   :type threshold: float, default=0.5
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryAccuracy
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = BinaryAccuracy()
   >>> metric(target, preds)
   0.75
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [1, 0, 1, 0]]
   >>> preds = [[0, 1, 1, 1], [1, 0, 1, 0]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.875















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the accuracy score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassAccuracy(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MulticlassStatScores`

   
   Compute the accuracy score for multiclass classification problems.

   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: Number of highest probability predictions or logits to consider when
                 computing the accuracy score.
   :type top_k: int, default=None
   :param average: If not None, this determines the type of averaging performed on the data:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each class, and find their unweighted
                       mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metrics for each class, and find their average,
                       weighted by support (the number of true instances for each class).
                       This alters ``macro`` to account for class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassAccuracy
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 0, 2, 2, 1]
   >>> metric = MulticlassAccuracy(num_classes=3)
   >>> metric(target, preds)
   array([1.        , 0.        , 0.66666667])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2], [2, 1, 0]]
   >>> preds = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.2, 0.6, 0.2]],
   ...          [[0.1, 0.8, 0.1], [0.05, 0.95, 0], [0.2, 0.6, 0.2]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0., 1., 0.])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the accuracy score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelAccuracy(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MultilabelStatScores`

   
   Compute the accuracy score for multilabel-indicator targets.

   :param num_labels: Number of labels in the multilabel classification task.
   :type num_labels: int
   :param threshold: Threshold value for binarizing the output of the classifier.
   :type threshold: float, default=0.5
   :param top_k: The number of highest probability or logit predictions considered
                 to find the correct label. Only works when ``preds`` contains
                 probabilities/logits.
   :type top_k: int, optional, default=None
   :param average: If None, return the accuracy score per label, otherwise this determines
                   the type of averaging performed on the data:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their
                       average, weighted by support (the number of true instances for
                       each label).
   :type average: Literal['micro', 'macro', 'weighted', None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal['warn', 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelAccuracy
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0, 1, 0], [1, 0, 1]]
   >>> metric = MultilabelAccuracy(num_labels=3)
   >>> metric(target, preds)
   array([1., 1., 0.])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 1], [1, 0, 0]], [[1, 0, 0], [0, 1, 1]]]
   >>> preds = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1]],
   ...          [[0.1, 0.8, 0.1], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.5, 0.5, 0.5])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the accuracy score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: AUROC

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the AUROC curve for different types of classification tasks.

   :param task: Task type. One of ``binary``, ``multiclass``, ``multilabel``.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param max_fpr: The maximum value of the false positive rate. If not None, a partial AUC
                   in the range [0, max_fpr] is returned. Only used for binary classification.
   :type max_fpr: float, default=None
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param num_classes: Number of classes. This parameter is required for the ``multiclass`` task.
   :type num_classes: int, default=None
   :param num_labels: Number of labels. This parameter is required for the ``multilabel`` task.
   :type num_labels: int, default=None
   :param average: If not None, apply the method to compute the average area under the
                   ROC curve. Only applicable for the ``multiclass`` and ``multilabel``
                   tasks. One of:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their average,
                       weighted by support (accounting for label imbalance).
   :type average: Literal["micro", "macro", "weighted"], default=None

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import BinaryAUROC
   >>> target = [0, 0, 1, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> metric = BinaryAUROC()
   >>> metric(target, preds)
   0.75
   >>> metric.reset_state()
   >>> target = [[0, 1, 0], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.7, 0.2, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.6111111111111112

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import MulticlassAUROC
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.89, 0.06],
   ...         [0.05, 0.01, 0.94], [0.9, 0.05, 0.05]]
   >>> metric = MulticlassAUROC(num_classes=3)
   >>> metric(target, preds)
   array([1., 1., 1.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 0], [1, 0, 1]]
   >>> preds = [[[0.1, 0.9, 0.0], [0.7, 0.2, 0.1], [0.2, 0.3, 0.5]],
   ...         [[0.1, 0.1, 0.8], [0.7, 0.2, 0.1], [0.2, 0.3, 0.5]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.5       , 0.22222222, 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import MultilabelAUROC
   >>> target = [[0, 1], [1, 1], [1, 0]]
   >>> preds = [[0.9, 0.05], [0.05, 0.89], [0.05, 0.01]]
   >>> metric = MultilabelAUROC(num_labels=2)
   >>> metric(target, preds)
   array([1., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 0]], [[1, 1], [1, 0]]]
   >>> preds = [[[0.9, 0.05], [0.05, 0.89]], [[0.05, 0.89], [0.05, 0.01]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1.   , 0.625])















   ..
       !! processed by numpydoc !!

.. py:class:: BinaryAUROC(max_fpr: float = None, thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1)

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall_curve.BinaryPrecisionRecallCurve`

   
   Compute the area under the ROC curve for binary classification tasks.

   :param max_fpr: The maximum value of the false positive rate. If not None, then
                   the partial AUCROC in the range [0, max_fpr] is returned.
   :type max_fpr: float, default=None
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryAUROC
   >>> target = [0, 0, 1, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> metric = BinaryAUROC()
   >>> metric(target, preds)
   0.75
   >>> metric.reset_state()
   >>> target = [[0, 1, 0], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.7, 0.2, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.6111111111111112















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the area under the ROC curve from the state variables.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassAUROC(num_classes: int, thresholds: Union[int, List[float], numpy.ndarray] = None, average: Literal[macro, weighted] = None)

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall_curve.MulticlassPrecisionRecallCurve`

   
   Compute the area under the ROC curve for multiclass classification tasks.

   :param num_classes: Number of classes.
   :type num_classes: int
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None
   :param average: If ``None``, then the scores for each class are returned. Otherwise,
                   this determines the type of averaging performed on the scores. One of:

                   - `macro`: Calculate metrics for each class, and find their unweighted
                       mean. This does not take class imbalance into account.
                   - `weighted`: Calculate metrics for each class, and find their average,
                       weighted by support (the number of true instances for each class).
   :type average: Literal["macro", "weighted"], default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassAUROC
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.89, 0.06],
   ...         [0.05, 0.01, 0.94], [0.9, 0.05, 0.05]]
   >>> metric = MulticlassAUROC(num_classes=3)
   >>> metric(target, preds)
   array([1., 1., 1.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 0], [1, 0, 1]]
   >>> preds = [[[0.1, 0.9, 0.0], [0.7, 0.2, 0.1], [0.2, 0.3, 0.5]],
   ...         [[0.1, 0.1, 0.8], [0.7, 0.2, 0.1], [0.2, 0.3, 0.5]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.5       , 0.22222222, 0.        ])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> Union[float, numpy.ndarray]

      
      Compute the area under the ROC curve from the state variables.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelAUROC(num_labels: int, thresholds: Union[int, List[float], numpy.ndarray] = None, average: Literal[micro, macro, weighted] = None)

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall_curve.MultilabelPrecisionRecallCurve`

   
   Compute the area under the ROC curve for multilabel classification tasks.

   :param num_labels: The number of labels in the multilabel classification task.
   :type num_labels: int
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None
   :param average: If ``None``, then the scores for each label are returned. Otherwise,
                   this determines the type of averaging performed on the scores. One of:

                   - `micro`: Calculate metrics globally.
                   - `macro`: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - `weighted``: Calculate metrics for each label, and find their average,
                       weighted by support (the number of true instances for each label).
   :type average: Literal["micro", "macro", "weighted"], default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelAUROC
   >>> target = [[0, 1], [1, 1], [1, 0]]
   >>> preds = [[0.9, 0.05], [0.05, 0.89], [0.05, 0.01]]
   >>> metric = MultilabelAUROC(num_labels=2)
   >>> metric(target, preds)
   array([1., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 0]], [[1, 1], [1, 0]]]
   >>> preds = [[[0.9, 0.05], [0.05, 0.89]], [[0.05, 0.89], [0.05, 0.01]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1.   , 0.625])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> Union[float, numpy.ndarray]

      
      Compute the area under the ROC curve from the state variables.
















      ..
          !! processed by numpydoc !!


.. py:class:: BinaryF1Score(pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`BinaryFbetaScore`

   
   Compute the F1 score for binary classification tasks.

   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold value for binarizing predictions in form of logits or
                     probability scores.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryF1Score
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.8, 0.4, 0.3]
   >>> metric = BinaryF1Score()
   >>> metric(target, preds)
   0.6666666666666666
   >>> metric.reset_state()
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.8















   ..
       !! processed by numpydoc !!

.. py:class:: BinaryFbetaScore(beta: float, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.BinaryStatScores`

   
   Compute the F-beta score for binary classification tasks.

   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param pos_label: The positive class label. One of [0, 1].
   :type pos_label: int, default=1
   :param threshold: Threshold value for converting probabilities and logits to binary.
                     Logits will be converted to probabilities using the sigmoid function.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there are no true positives or true negatives.
                         If set to ``warn``, this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryFbetaScore
   >>> target = [0, 1, 1, 0]
   >>> preds = [0, 1, 0, 0]
   >>> metric = BinaryFbetaScore(beta=0.5)
   >>> metric(target, preds)
   0.8333333333333334
   >>> metric.reset_state()
   >>> target = [[1, 0, 1, 0], [1, 0, 0, 1]]
   >>> preds = [[0.2, 0.8, 0.3, 0.4], [0.6, 0.3, 0.1, 0.5]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.625















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the metric from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: F1Score

   Bases: :py:obj:`FbetaScore`

   
   Compute the F1 score for different types of classification tasks.

   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import F1Score
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.8, 0.4, 0.3]
   >>> metric = F1Score(task="binary")
   >>> metric(target, preds)
   0.6666666666666666
   >>> metric.reset_state()
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.8

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import F1Score
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.95, 0.05, 0]]
   >>> metric = F1Score(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([0.66666667, 0.5       , 0.        ])
   >>> metric.reset_state()
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[[0.1, 0.9, 0], [0.05, 0.95, 0]],
   ...         [[0.1, 0.8, 0.1], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.        , 0.85714286, 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import F1Score
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.2]]
   >>> metric = F1Score(task="multilabel", num_labels=3)
   >>> metric(target, preds)
   array([0., 1., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7, 0.2], [0.2, 0.8, 0.3]],
   ...     [[0.5, 0.9, 0.0], [0.3, 0.4, 0.2]],
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0. , 0.8, 0. ])















   ..
       !! processed by numpydoc !!

.. py:class:: FbetaScore

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the F-beta score for different types of classification tasks.

   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import FbetaScore
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.8, 0.4, 0.3]
   >>> metric = FbetaScore(beta=0.5, task="binary")
   >>> metric(target, preds)
   0.8333333333333334
   >>> metric.reset_state()
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.9090909090909091

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import FbetaScore
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.1, 0.8, 0.1], [0.1, 0.1, 0.8], [0.1, 0.1, 0.8], [0.8, 0.1, 0.1]]
   >>> metric = FbetaScore(beta=0.5, task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([0.83333333, 0.        , 0.55555556])
   >>> metric.reset_state()
   >>> target = [[0, 1, 0], [0, 0, 1]]
   >>> preds = [[[0.1, 0.8, 0.1], [0.1, 0.1, 0.8], [0.8, 0.1, 0.1]],
   ...          [[0.1, 0.1, 0.8], [0.8, 0.1, 0.1], [0.1, 0.8, 0.1]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.83333333, 0.5       , 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import FbetaScore
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> metric = FbetaScore(beta=0.5, task="multilabel", num_labels=2)
   >>> metric(target, preds)
   array([1.        , 0.83333333])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [[[0, 1], [1, 0]], [[1, 1], [1, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1.        , 0.90909091])















   ..
       !! processed by numpydoc !!

.. py:class:: MulticlassF1Score(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`MulticlassFbetaScore`

   
   Compute the F1 score for multiclass classification tasks.

   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each class. Otherwise, use one of
                   the following options to compute the average score:

                   - ``micro``: Calculate metric globally.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class
                       imbalance. It can result in an F-score that is not between
                       precision and recall.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassF1Score
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.95, 0.05, 0]]
   >>> metric = MulticlassF1Score(num_classes=3)
   >>> metric(target, preds)
   array([0.66666667, 0.5       , 0.        ])
   >>> metric.reset_state()
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[[0.1, 0.9, 0], [0.05, 0.95, 0]],
   ...         [[0.1, 0.8, 0.1], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.        , 0.85714286, 0.        ])















   ..
       !! processed by numpydoc !!

.. py:class:: MulticlassFbetaScore(beta: float, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MulticlassStatScores`

   
   Compute the F-beta score for multiclass classification tasks.

   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param num_classes: The number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassFbetaScore
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> metric = MulticlassFbetaScore(beta=0.5, num_classes=3)
   >>> metric(target, preds)
   array([1., 0., 0.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.90909091, 0.        , 0.        ])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the metric from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelF1Score(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`MultilabelFbetaScore`

   
   Compute the F1 score for multilabel classification tasks.

   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each label. Otherwise, use one of
                   the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false positives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelF1Score
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.2]]
   >>> metric = MultilabelF1Score(num_labels=3)
   >>> metric(target, preds)
   array([0., 1., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7, 0.2], [0.2, 0.8, 0.3]],
   ...     [[0.5, 0.9, 0.0], [0.3, 0.4, 0.2]],
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0. , 0.8, 0. ])















   ..
       !! processed by numpydoc !!

.. py:class:: MultilabelFbetaScore(beta: float, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MultilabelStatScores`

   
   Compute the F-beta score for multilabel classification tasks.

   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class if predicitions are logits
                     or probability scores. Logits will be converted to probabilities using
                     the sigmoid function.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each label. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelFbetaScore
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> metric = MultilabelFbetaScore(beta=0.5, num_labels=2)
   >>> metric(target, preds)
   array([1.        , 0.83333333])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [[[0, 1], [1, 0]], [[1, 1], [1, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1.        , 0.90909091])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the metric from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MetricCollection(metrics: Union[Metric, Sequence[Metric], Dict[str, Metric]])

   Bases: :py:obj:`Metric`

   
   A collection of metrics.

   Provides a convenient way to compute multiple metrics at once. It groups
   metrics that have similar state variables and only updates the state variables
   once per group, reducing the amount of computation required.

   :param metrics: The list of metrics to collect.
   :type metrics: List[Metric]















   ..
       !! processed by numpydoc !!
   .. py:method:: add_state(*args: Any, **kwargs: Any) -> None
      :abstractmethod:

      
      Add state variables to the metric.

      Not implemented for ``MetricCollection``.















      ..
          !! processed by numpydoc !!

   .. py:method:: update_state(*args: Any, **kwargs: Any) -> None

      
      Update the state of all metrics in the collection.

      Uses the metric groups to only update the state variables once per group.

      :param \*args: The positional arguments to pass to the update_state method of each metric.
      :type \*args: Any
      :param \*\*kwargs: The keyword arguments to pass to the update_state method of each metric.
      :type \*\*kwargs: Any















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> Dict[str, Any]

      
      Compute the metrics in the collection.
















      ..
          !! processed by numpydoc !!

   .. py:method:: reset_state() -> None

      
      Reset the state of all metrics in the collection.
















      ..
          !! processed by numpydoc !!

   .. py:method:: _validate_input(metrics: Union[Metric, Sequence[Metric], Dict[str, Metric]])

      
      Validate the input to the constructor.

      :param metrics: The input to the constructor.
      :type metrics: Union[Metric, Sequence[Metric], Dict[str, Metric]]

      :raises TypeError: If the input is not a metric, sequence of metrics, or dictionary of metrics.















      ..
          !! processed by numpydoc !!

   .. py:method:: _has_same_update_state_parameters_check(metric_a: Metric, metric_b: Metric) -> None
      :staticmethod:

      
      Check if two metrics have the same ``update_state`` method parameters.

      :param metric_a: The first metric.
      :type metric_a: Metric
      :param metric_b: The second metric.
      :type metric_b: Metric

      :rtype: None

      :raises ValueError: If the metrics do not have the same signature for their ``update_state``
          method.















      ..
          !! processed by numpydoc !!

   .. py:method:: _has_same_task_type_check(metric_a: Metric, metric_b: Metric) -> None
      :staticmethod:

      
      Check if two metrics are for the same task.

      :param metric_a: The first metric.
      :type metric_a: Metric
      :param metric_b: The second metric.
      :type metric_b: Metric

      :rtype: None

      :raises ValueError: If the metrics are not for the same task. If both metrics are for
          multilabel tasks, then they must have the same ``num_labels``
          parameter. If both metrics are for multiclass tasks, then they must
          have the same ``num_classes`` parameter.















      ..
          !! processed by numpydoc !!

   .. py:method:: _get_metric_groups() -> Dict[int, List[str]]

      
      Group metrics by the state variables they use.

      :returns: **metric_groups** -- A dictionary with the group id as the key and a list of metric names
                as the value.
      :rtype: Dict[int, List[str]]















      ..
          !! processed by numpydoc !!

   .. py:method:: __call__(*args, **kwargs)

      
      Update the global metric state and compute the metric for a batch.
















      ..
          !! processed by numpydoc !!


.. py:function:: create_metric(metric_name: str, **kwargs: Optional[Dict[str, Any]]) -> Metric

   
   Create a metric instance from a name.

   :param metric_name: The name of the metric.
   :type metric_name: str
   :param \*\*kwargs: The keyword arguments to pass to the metric constructor.
   :type \*\*kwargs: Optional[Dict[str, Any]]

   :returns: **metric** -- The metric instance.
   :rtype: Metric















   ..
       !! processed by numpydoc !!

.. py:class:: BinaryPrecision(pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.BinaryStatScores`

   
   Compute the precision score for binary classification tasks.

   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryPrecision
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = BinaryPrecision()
   >>> metric(target, preds)
   0.6666666666666666
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.6666666666666666















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the precision score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: BinaryRecall(pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.BinaryStatScores`

   
   Computes recall score for binary classification.

   :param pos_label: Label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryRecall
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 0]
   >>> metric = Recall()
   >>> metric(target, preds)
   0.5
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.5















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the recall score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassPrecision(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MulticlassStatScores`

   
   Compute the precision score for multiclass classification tasks.

   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each class. Otherwise, use one of the
                   following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false positives.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassPrecision
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> metric = MulticlassPrecision(num_classes=3, average=None)
   >>> metric(target, preds)
   array([1. , 0. , 0.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1., 0., 0.])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the precision score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassRecall(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MulticlassStatScores`

   
   Compute the recall score for multiclass classification tasks.

   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the recall will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the recall score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false negatives.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassRecall
   >>> target = [0, 1, 2, 0]
   >>> preds = [2, 0, 2, 1]
   >>> metric = MulticlassRecall(num_classes=3)
   >>> metric(target, preds)
   array([0., 0., 1.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.66666667, 0.        , 0.        ])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the recall score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelPrecision(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MultilabelStatScores`

   
   Compute the precision score for multilabel classification tasks.

   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the precision score for each label. Otherwise,
                   use one of the following options to compute the average precision score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false positives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelPrecision
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.2, 0.8]]
   >>> metric = MultilabelPrecision(num_labels=2, average=None)
   >>> metric(target, preds)
   array([0., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7], [0.2, 0.8]],
   ...     [[0.5, 0.9], [0.3, 0.4]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1., 1.])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the precision score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelRecall(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MultilabelStatScores`

   
   Compute the recall score for multilabel classification tasks.

   :param num_labels: Number of labels in the dataset.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param average: If ``None``, return the score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelRecall
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> metric = MultilabelRecall(num_labels=4)
   >>> metric(target, preds)
   array([0., 1., 1. , 0. ])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 0, 1], [0, 0, 1, 1]], [[0, 1, 0, 1], [0, 0, 1, 1]]]
   >>> preds = [[[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]],
   ...          [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0., 1., 1., 0.])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the recall score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: Precision

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the precision score for different types of classification tasks.

   This metric can be used for binary, multiclass, and multilabel classification
   tasks. It creates the appropriate metric based on the ``task`` parameter.

   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the precision score for each label/class. Otherwise,
                   use one of the following options to compute the average precision score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and false positives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   (binary)
   >>> from cyclops.evaluation.metrics import Precision
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = Precision(task="binary")
   >>> metric(target, preds)
   0.6666666666666666
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.6666666666666666

   (multiclass)
   >>> from cyclops.evaluation.metrics import Precision
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> metric = Precision(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([1. , 0. , 0.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1., 0., 0.])

   (multilabel)
   >>> from cyclops.evaluation.metrics import Precision
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.2, 0.8]]
   >>> metric = Precision(task="multilabel", num_labels=2)
   >>> metric.update_state(target, preds)
   >>> metric.compute()
   array([0., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7], [0.2, 0.8]],
   ...     [[0.5, 0.9], [0.3, 0.4]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1., 1.])















   ..
       !! processed by numpydoc !!

.. py:class:: Recall

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the recall score for different types of classification tasks.

   This metric can be used for binary, multiclass, and multilabel classification
   tasks. It creates the appropriate class based on the ``task`` parameter.

   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the recall score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import Recall
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = Recall(task="binary")
   >>> metric(target, preds)
   1.
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.5

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import Recall
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> metric = Recall(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([1. , 0. , 0.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.66666667, 0.        , 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import Recall
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.2, 0.8]]
   >>> metric = Recall(task="multilabel", num_labels=2)
   >>> metric(target, preds)
   array([0., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7], [0.2, 0.8]],
   ...     [[0.5, 0.9], [0.3, 0.4]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.33333333, 1.        ])















   ..
       !! processed by numpydoc !!

.. py:class:: BinaryPrecisionRecallCurve(thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1)

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute precision-recall curve for binary input.

   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list or numpy.ndarray, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param pos_label: The label of the positive class.
   :type pos_label: int

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryPrecisionRecallCurve
   >>> target = [0, 1, 0, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> metric = BinaryPrecisionRecallCurve(thresholds=3)
   >>> metric(target, preds)
   (array([0.5, 1. , 0. ]), array([1. , 0.5, 0. ]), array([0. , 0.5, 1. ]))
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [1, 1, 0, 0]]
   >>> preds = [[0.1, 0.4, 0.35, 0.8], [0.6, 0.3, 0.1, 0.7]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([0.5       , 0.66666667, 0.        ]),
   array([1. , 0.5, 0. ]),
   array([0. , 0.5, 1. ]))















   ..
       !! processed by numpydoc !!
   .. py:method:: update_state(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike) -> None

      
      Update the state of the metric.

      The state is either a list of targets and predictions (if ``thresholds`` is
      ``None``) or a confusion matrix.















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]

      
      Compute the precision-recall curve from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassPrecisionRecallCurve(num_classes: int, thresholds: Union[int, List[float], numpy.ndarray] = None)

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the precision-recall curve for multiclass problems.

   :param num_classes: The number of classes in the dataset.
   :type num_classes: int
   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassPrecisionRecallCurve
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0.],
   ...          [0.5, 0.3, 0.2], [0.2, 0.5, 0.3]]
   >>> metric = MulticlassPrecisionRecallCurve(num_classes=3, thresholds=3)
   >>> metric(target, preds)
   (array([[0.5       , 0.        , 0.        , 1.        ],
           [0.25      , 0.33333333, 0.        , 1.        ],
           [0.25      , 0.        , 0.        , 1.        ]]),
   array([[1., 0., 0., 0.],
           [1., 1., 0., 0.],
           [1., 0., 0., 0.]]),
   array([0. , 0.5, 1. ]))
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [1, 2, 0, 1]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3], [0.05, 0.95, 0.], [0.5, 0.3, 0.2], [0.2, 0.5, 0.3]],
   ...     [[0.3, 0.2, 0.5], [0.1, 0.7, 0.2], [0.6, 0.1, 0.3], [0.1, 0.8, 0.1]],
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0.375, 0.5  , 0.   , 1.   ],
           [0.375, 0.4  , 0.   , 1.   ],
           [0.25 , 0.   , 0.   , 1.   ]]),
       array([[1.        , 0.33333333, 0.        , 0.        ],
           [1.        , 0.66666667, 0.        , 0.        ],
           [1.        , 0.        , 0.        , 0.        ]]),
       array([0. , 0.5, 1. ]))















   ..
       !! processed by numpydoc !!
   .. py:method:: update_state(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike) -> None

      
      Update the state of the metric.

      The state is either a list of targets and predictions (if ``thresholds`` is
      ``None``) or a confusion matrix.















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> Union[Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray], Tuple[List[numpy.ndarray], List[numpy.ndarray], List[numpy.ndarray]]]

      
      Compute the precision-recall curve from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelPrecisionRecallCurve(num_labels: int, thresholds: Union[int, List[float], numpy.ndarray] = None)

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Check and format the multilabel precision-recall curve input/data.

   :param num_labels: The number of labels in the dataset.
   :type num_labels: int
   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int, list of floats or numpy.ndarray of floats, default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelPrecisionRecallCurve
   >>> target = [[0, 1], [1, 0]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> metric = MultilabelPrecisionRecallCurve(num_labels=2, thresholds=3)
   >>> metric(target, preds)
   (array([[0.5, 1. , 0. , 1. ],
           [0.5, 1. , 0. , 1. ]]),
   array([[1., 1., 0., 0.],
           [1., 1., 0., 0.]]),
   array([0. , 0.5, 1. ]))
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 0]], [[1, 0], [0, 1]]]
   >>> preds = [[[0.1, 0.9], [0.8, 0.2]], [[0.2, 0.8], [0.7, 0.3]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0.5, 0.5, 0. , 1. ],
           [0.5, 0.5, 0. , 1. ]]),
   array([[1. , 0.5, 0. , 0. ],
           [1. , 0.5, 0. , 0. ]]),
   array([0. , 0.5, 1. ]))















   ..
       !! processed by numpydoc !!
   .. py:method:: update_state(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike) -> None

      
      Update the state of the metric.

      The state is either a list of targets and predictions (if ``thresholds`` is
      ``None``) or a confusion matrix.















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]

      
      Compute the precision-recall curve from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: PrecisionRecallCurve

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the precision-recall curve for different classification tasks.

   :param task: The task for which the precision-recall curve is computed.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param thresholds: Thresholds used for computing the precision and recall scores. If int,
                      then the number of thresholds to use. If list or array, then the
                      thresholds to use. If None, then the thresholds are automatically
                      determined by the sunique values in ``preds``
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: The number of classes in the dataset. Required if ``task`` is
                       ``"multiclass"``.
   :type num_classes: int, optional
   :param num_labels: The number of labels in the dataset. Required if ``task`` is
                      ``"multilabel"``.
   :type num_labels: int, optional

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import PrecisionRecallCurve
   >>> target = [1, 1, 1, 0]
   >>> preds = [0.6, 0.2, 0.3, 0.8]
   >>> metric = PrecisionRecallCurve(task="binary", thresholds=None)
   >>> metric(target, preds)
   (array([0.75      , 0.66666667, 0.5       , 0.        , 1.        ]),
   array([1.        , 0.66666667, 0.33333333, 0.        , 0.        ]),
   array([0.2, 0.3, 0.6, 0.8]))
   >>> metric.reset_state()
   >>> target = [[1, 0, 1, 1], [0, 0, 0, 1]]
   >>> preds = [[0.5, 0.4, 0.1, 0.3], [0.9, 0.6, 0.45, 0.8]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([0.5       , 0.42857143, 0.33333333, 0.4       , 0.5       ,
       0.33333333, 0.5       , 0.        , 1.        ]),
   array([1.  , 0.75, 0.5 , 0.5 , 0.5 , 0.25, 0.25, 0.  , 0.  ]),
   array([0.1 , 0.3 , 0.4 , 0.45, 0.5 , 0.6 , 0.8 , 0.9 ]))

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import PrecisionRecallCurve
   >>> target = [0, 1, 2, 2]
   >>> preds = [[0.05, 0.95, 0], [0.1, 0.8, 0.1],
   ...         [0.2, 0.2, 0.6], [0.2, 0.2, 0.6]]
   >>> metric = PrecisionRecallCurve(task="multiclass", num_classes=3,
   ...     thresholds=3)
   >>> metric(target, preds)
   (array([[0.25, 0.  , 0.  , 1.  ],
           [0.25, 0.5 , 0.  , 1.  ],
           [0.5 , 1.  , 0.  , 1.  ]]),
   array([[1., 0., 0., 0.],
           [1., 1., 0., 0.],
           [1., 1., 0., 0.]]),
   array([0. , 0.5, 1. ]))
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 2], [1, 2, 0, 1]]
   >>> preds = [[[0.05, 0.95, 0], [0.1, 0.8, 0.1],
   ...         [0.2, 0.2, 0.6], [0.2, 0.2, 0.6]],
   ...         [[0.05, 0.95, 0], [0.1, 0.8, 0.1],
   ...         [0.2, 0.2, 0.6], [0.2, 0.2, 0.6]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0.25 , 0.   , 0.   , 1.   ],
           [0.375, 0.5  , 0.   , 1.   ],
           [0.375, 0.5  , 0.   , 1.   ]]),
   array([[1.        , 0.        , 0.        , 0.        ],
           [1.        , 0.66666667, 0.        , 0.        ],
           [1.        , 0.66666667, 0.        , 0.        ]]),
   array([0. , 0.5, 1. ]))

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import PrecisionRecallCurve
   >>> target = [[0, 1], [1, 0]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> metric = PrecisionRecallCurve(task="multilabel", num_labels=2,
   ...     thresholds=3)
   >>> metric(target, preds)
   (array([[0.5, 1. , 0. , 1. ],
           [0.5, 1. , 0. , 1. ]]),
   array([[1., 1., 0., 0.],
           [1., 1., 0., 0.]]),
   array([0. , 0.5, 1. ]))
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 0]], [[1, 0], [0, 1]]]
   >>> preds = [[[0.1, 0.9], [0.8, 0.2]],
   ...         [[0.1, 0.9], [0.8, 0.2]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0.5, 0.5, 0. , 1. ],
           [0.5, 0.5, 0. , 1. ]]),
   array([[1. , 0.5, 0. , 0. ],
           [1. , 0.5, 0. , 0. ]]),
   array([0. , 0.5, 1. ]))















   ..
       !! processed by numpydoc !!

.. py:class:: BinaryROCCurve(thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1)

   Bases: :py:obj:`cyclops.evaluate.metrics.BinaryPrecisionRecallCurve`

   
   Compute the ROC curve for binary classification tasks.

   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryROCCurve
   >>> target = [0, 0, 1, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> metric = BinaryROCCurve()
   >>> metric(target, preds)
   (array([0. , 0. , 0.5, 0.5, 1. ]),
   array([0. , 0.5, 0.5, 1. , 1. ]),
   array([1.  , 0.8 , 0.4 , 0.35, 0.1 ]))
   >>> metric.reset_state()
   >>> target = [[1, 1, 0, 0], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.2, 0.3, 0.4], [0.6, 0.5, 0.4, 0.3]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([0.  , 0.25, 0.5 , 0.75, 1.  , 1.  , 1.  ]),
   array([0.  , 0.  , 0.  , 0.25, 0.5 , 0.75, 1.  ]),
   array([1. , 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]))















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]

      
      Compute the ROC curve from the state variables.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassROCCurve(num_classes: int, thresholds: Union[int, List[float], numpy.ndarray] = None)

   Bases: :py:obj:`cyclops.evaluate.metrics.MulticlassPrecisionRecallCurve`

   
   Compute the ROC curve for multiclass classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If ``preds`` is not in
                 the range [0, 1], a softmax function is applied to transform it to
                 the range [0, 1].
   :type preds: ArrayLike
   :param num_classes: Number of classes.
   :type num_classes: int
   :param thresholds: Thresholds used for binarizing the predicted probabilities.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassROCCurve
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.05, 0.95, 0], [0.1, 0.8, 0.1],
   ...         [0.2, 0.2, 0.6], [0.9, 0.1, 0]]
   >>> metric = MulticlassROCCurve(num_classes=3, thresholds=4)
   >>> metric(target, preds)
   (array([[0.        , 0.        , 0.        , 1.        ],
           [0.        , 0.33333333, 0.33333333, 1.        ],
           [0.        , 0.        , 0.        , 1.        ]]),
   array([[0. , 0.5, 0.5, 1. ],
           [0. , 1. , 1. , 1. ],
           [0. , 0. , 1. , 1. ]]),
   array([1.        , 0.66666667, 0.33333333, 0.        ]))
   >>> metric.reset_state()
   >>> target = [[1, 1, 0, 0], [0, 0, 1, 1]]
   >>> preds = [[[0.1, 0.2, 0.7], [0.5, 0.4, 0.1],
   ...         [0.2, 0.3, 0.5], [0.8, 0.1, 0.1]],
   ...         [[0.1, 0.2, 0.7], [0.5, 0.4, 0.1],
   ...         [0.2, 0.3, 0.5], [0.8, 0.1, 0.1]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0.  , 0.25, 0.5 , 1.  ],
           [0.  , 0.  , 0.25, 1.  ],
           [0.  , 0.25, 0.5 , 1.  ]]),
   array([[0.  , 0.25, 0.5 , 1.  ],
           [0.  , 0.  , 0.25, 1.  ],
           [0.  , 0.  , 0.  , 0.  ]]),
   array([1.        , 0.66666667, 0.33333333, 0.        ]))















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]

      
      Compute the ROC curve from the state variables.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelROCCurve(num_labels: int, thresholds: Union[int, List[float], numpy.ndarray] = None)

   Bases: :py:obj:`cyclops.evaluate.metrics.MultilabelPrecisionRecallCurve`

   
   Compute the ROC curve for multilabel classification tasks.

   :param num_labels: The number of labels in the dataset.
   :type num_labels: int
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelROCCurve
   >>> target = [[1, 1, 0], [0, 1, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
   >>> metric = MultilabelROCCurve(num_labels=3, thresholds=4)
   >>> metric(target, preds)
   (array([[0. , 0. , 0. , 1. ],
           [0. , 0. , 0. , 0. ],
           [0. , 0.5, 0.5, 1. ]]),
   array([[0., 0., 0., 1.],
           [0., 1., 1., 1.],
           [0., 0., 0., 0.]]),
   array([1.        , 0.66666667, 0.33333333, 0.        ]))
   >>> metric.reset_state()
   >>> target = [[[1, 1, 0], [0, 1, 0]], [[1, 1, 0], [0, 1, 0]]]
   >>> preds = [[[0.1, 0.9, 0.8], [0.05, 0.95, 0]],
   ...         [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0. , 0. , 0. , 1. ],
           [0. , 0. , 0. , 0. ],
           [0. , 0.5, 0.5, 1. ]]),
   array([[0., 0., 0., 1.],
           [0., 1., 1., 1.],
           [0., 0., 0., 0.]]),
   array([1.        , 0.66666667, 0.33333333, 0.        ]))















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]

      
      Compute the ROC curve from the state variables.
















      ..
          !! processed by numpydoc !!


.. py:class:: ROCCurve

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the ROC curve for different types of classification tasks.

   :param task: The type of task for the input data. One of 'binary', 'multiclass'
                or 'multilabel'.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param thresholds: Thresholds used for computing the ROC curve. Can be one of:

                      - None: use the unique values of ``preds`` as thresholds
                      - int: generate ``thresholds`` number of evenly spaced values between
                          0 and 1 as thresholds.
                      - list of floats: use the values in the list as thresholds. The list
                          of values should be monotonically increasing. The list will be
                          converted into a numpy array.
                      - numpy.ndarray of floats: use the values in the array as thresholds.
                          The array should be 1d and monotonically increasing.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: The number of classes in the dataset. Required if ``task`` is
                       ``"multiclass"``.
   :type num_classes: int, optional
   :param num_labels: The number of labels in the dataset. Required if ``task`` is
                      ``"multilabel"``.
   :type num_labels: int, optional

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import ROCCurve
   >>> target = [0, 0, 1, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> metric = ROCCurve(task="binary", thresholds=None)
   >>> metric(target, preds)
   (array([0. , 0. , 0.5, 0.5, 1. ]),
   array([0. , 0.5, 0.5, 1. , 1. ]),
   array([1.  , 0.8 , 0.4 , 0.35, 0.1 ]))
   >>> metric.reset_state()
   >>> target = [[1, 1, 0, 0], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.2, 0.3, 0.4], [0.6, 0.5, 0.4, 0.3]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([0.  , 0.25, 0.5 , 0.75, 1.  , 1.  , 1.  ]),
   array([0.  , 0.  , 0.  , 0.25, 0.5 , 0.75, 1.  ]),
   array([1. , 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]))

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import ROCCurve
   >>> target = [[1, 1, 0], [0, 1, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
   >>> metric = ROCCurve(task="multiclass", num_classes=3, thresholds=4)
   >>> metric(target, preds)
   (array([[0. , 0. , 0. , 1. ],
           [0. , 0. , 0. , 0. ],
           [0. , 0.5, 0.5, 1. ]]),
   array([[0., 0., 0., 1.],
           [0., 1., 1., 1.],
           [0., 0., 0., 0.]]),
   array([1.        , 0.66666667, 0.33333333, 0.        ]))
   >>> metric.reset_state()
   >>> target = [[[1, 1, 0], [0, 1, 0]], [[1, 1, 0], [0, 1, 0]]]
   >>> preds = [[[0.1, 0.9, 0.8], [0.05, 0.95, 0]],
   ...         [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0. , 0. , 0. , 1. ],
           [0. , 0. , 0. , 0. ],
           [0. , 0.5, 0.5, 1. ]]),
   array([[0., 0., 0., 1.],
           [0., 1., 1., 1.],
           [0., 0., 0., 0.]]),
   array([1.        , 0.66666667, 0.33333333, 0.        ]))

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import ROCCurve
   >>> target = [[1, 1, 0], [0, 1, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
   >>> metric = ROCCurve(task="multilabel", num_labels=3, thresholds=4)
   >>> metric(target, preds)
   (array([[0. , 0. , 0. , 1. ],
           [0. , 0. , 0. , 0. ],
           [0. , 0.5, 0.5, 1. ]]),
   array([[0., 0., 0., 1.],
           [0., 1., 1., 1.],
           [0., 0., 0., 0.]]),
   array([1.        , 0.66666667, 0.33333333, 0.        ]))
   >>> metric.reset_state()
   >>> target = [[[1, 1, 0], [0, 1, 0]], [[1, 1, 0], [0, 1, 0]]]
   >>> preds = [[[0.1, 0.9, 0.8], [0.05, 0.95, 0]],
   ...         [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   (array([[0. , 0. , 0. , 1. ],
           [0. , 0. , 0. , 0. ],
           [0. , 0.5, 0.5, 1. ]]),
   array([[0., 0., 0., 1.],
           [0., 1., 1., 1.],
           [0., 0., 0., 0.]]),
   array([1.        , 0.66666667, 0.33333333, 0.        ]))















   ..
       !! processed by numpydoc !!

.. py:class:: BinarySensitivity(pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall.BinaryRecall`

   
   Computes sensitivity score for binary classification.

   :param pos_label: Label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinarySensitivity
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 0]
   >>> metric = Sensitivity()
   >>> metric(target, preds)
   0.5
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.5















   ..
       !! processed by numpydoc !!

.. py:class:: MulticlassSensitivity(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall.MulticlassRecall`

   
   Compute the sensitivity score for multiclass classification tasks.

   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the sensitivity will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the sensitivity score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false negatives.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassSensitivity
   >>> target = [0, 1, 2, 0]
   >>> preds = [2, 0, 2, 1]
   >>> metric = MulticlassSensitivity(num_classes=3)
   >>> metric(target, preds)
   array([0., 0., 1.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.66666667, 0.        , 0.        ])















   ..
       !! processed by numpydoc !!

.. py:class:: MultilabelSensitivity(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall.MultilabelRecall`

   
   Compute the sensitivity score for multilabel classification tasks.

   :param num_labels: Number of labels in the dataset.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param average: If ``None``, return the score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelSensitivity
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> metric = MultilabelSensitivity(num_labels=4)
   >>> metric(target, preds)
   array([0., 1., 1. , 0. ])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 0, 1], [0, 0, 1, 1]], [[0, 1, 0, 1], [0, 0, 1, 1]]]
   >>> preds = [[[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]],
   ...          [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0., 1., 1., 0.])















   ..
       !! processed by numpydoc !!

.. py:class:: Sensitivity

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the sensitivity score for different types of classification tasks.

   This metric can be used for binary, multiclass, and multilabel classification
   tasks. It creates the appropriate class based on the ``task`` parameter.

   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the sensitivity score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import Sensitivity
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = Sensitivity(task="binary")
   >>> metric.update_state(target, preds)
   >>> metric.compute()
   1.
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.5

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import Sensitivity
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> metric = Sensitivity(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([1. , 0. , 0.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.66666667, 0.        , 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import Sensitivity
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.2, 0.8]]
   >>> metric = Sensitivity(task="multilabel", num_labels=2)
   >>> metric(target, preds)
   array([0., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7], [0.2, 0.8]],
   ...     [[0.5, 0.9], [0.3, 0.4]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.33333333, 1.        ])















   ..
       !! processed by numpydoc !!

.. py:class:: BinarySpecificity(pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.BinaryStatScores`

   
   Compute specificity for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param pos_label: The label to use for the positive class.
   :type pos_label: int, default=1
   :param threshold: The threshold to use for converting the predictions to binary
                     values. Logits will be converted to probabilities using the sigmoid
                     function.
   :type threshold: float, default=0.5

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinarySpecificity
   >>> target = [0, 1, 1, 0]
   >>> preds = [0, 1, 0, 0]
   >>> metric = BinarySpecificity()
   >>> metric(target, preds)
   1.0
   >>> metric.reset_state()
   >>> target = [[0, 1, 1, 0], [1, 1, 0, 0]]
   >>> preds = [[0, 1, 0, 0], [1, 0, 0, 0]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   1.0















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the specificity score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassSpecificity(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MulticlassStatScores`

   
   Compute specificity for multiclass classification tasks.

   :param num_classes: The number of classes in the dataset.
   :type num_classes: int
   :param top_k: Number of highest probability or logit score predictions considered
                 to find the correct label. Only works when ``preds`` contain
                 probabilities/logits.
   :type top_k: int, optional
   :param average: If None, return the specificity for each class, otherwise return the
                   average specificity. Average options are:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each class, and find their unweighted
                       mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metrics for each class, and find their
                       average, weighted by support (the number of true instances for each
                       label).
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassSpecificity
   >>> target = [0, 1, 2, 0, 1, 2]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]]
   >>> metric = MulticlassSpecificity(num_classes=3)
   >>> metric(target, preds)
   array([1.  , 0.75, 1.  ])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0, 1, 2], [1, 1, 2, 0, 0, 1]]
   >>> preds = [[0, 2, 1, 2, 0, 1], [1, 0, 1, 2, 2, 0]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.625     , 0.57142857, 0.55555556])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the specificity score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelSpecificity(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MultilabelStatScores`

   
   Compute specificity for multilabel classification tasks.

   :param num_labels: The number of labels in the dataset.
   :type num_labels: int
   :param threshold: The threshold value for converting probability or logit scores to
                     binary. A sigmoid function is first applied to logits to convert them
                     to probabilities.
   :type threshold: float, default=0.5
   :param top_k: Number of highest probability or logit score predictions considered
                 to find the correct label. Only works when ``preds`` contains
                 probabilities/logits.
   :type top_k: int, optional
   :param average: If None, return the specificity for each class, otherwise return the
                   average specificity. Average options are:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their average,
                       weighted by support (the number of true instances for each label).
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelSpecificity
   >>> target = [[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 0, 1], [1, 0, 0]]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05]]
   >>> metric = MultilabelSpecificity(num_labels=3)
   >>> metric(target, preds)
   array([0.5, 0. , 0.5])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 0, 1], [1, 0, 0]],
   ...           [[1, 0, 1], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 0]]]
   >>> preds = [[[1, 0, 0], [0, 1, 0], [0, 1, 1], [0, 0, 1], [1, 0, 0]],
   ...          [[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 0, 1], [1, 0, 0]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.5       , 0.66666667, 0.6       ])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the specificity score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: Specificity

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute specificity score for different classification tasks.

   The specificity is the ratio of true negatives to the sum of true negatives and
   false positives. It is also the recall of the negative class.

   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import Specificity
   >>> target = [0, 1, 1, 0, 1]
   >>> preds = [0.9, 0.05, 0.05, 0.35, 0.05]
   >>> metric = Specificity(task="binary")
   >>> metric(target, preds)
   0.5
   >>> metric.reset_state()
   >>> target = [[0, 1, 1], [1, 0, 1]]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.0

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import Specificity
   >>> target = [0, 1, 2, 0, 1, 2]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]]
   >>> metric = Specificity(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([1.  , 0.75, 1.  ])
   >>> metric.reset_state()
   >>> target = [[0, 1, 1], [1, 2, 1]]
   >>> preds = [[[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75]],
   ...          [[0.35, 0.5, 0.15], [0.25, 0.5, 0.25], [0.5, 0.05, 0.45]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.8, 0.5, 0.8])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import Specificity
   >>> target = [[0, 1, 1], [1, 0, 1], [1, 1, 0]]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.2, 0.75], [0.35, 0.5, 0.15]]
   >>> metric = Specificity(task="multilabel", num_labels=3)
   >>> metric(target, preds)
   array([0., 1., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7, 0.2], [0.2, 0.8, 0.3]],
   ...     [[0.5, 0.9, 0.0], [0.3, 0.4, 0.2]],
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.5, 0.5, 1. ])















   ..
       !! processed by numpydoc !!

.. py:class:: BinaryStatScores(pos_label: int = 1, threshold: float = 0.5)

   Bases: :py:obj:`_AbstractScores`

   
   Compute binary stat scores.

   :param pos_label: The label to use for the positive class.
   :type pos_label: int, default=1
   :param threshold: The threshold to use for converting the predictions to binary
                     values. Logits will be converted to probabilities using the sigmoid
                     function.
   :type threshold: float, default=0.5

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryStatScores
   >>> target = [0, 1, 1, 0]
   >>> preds = [0, 1, 0, 0]
   >>> metric = BinaryStatScores(threshold=0.5, pos_label=1)
   >>> metric(target=target, preds=preds)
   array([1, 0, 2, 1, 2])
   >>> metric.reset_state()
   >>> target = [[1, 1, 0, 1, 0, 0], [0, 0, 1, 1, 0, 0]]
   >>> preds = [[0.9, 0.8, 0.3, 0.4, 0.5, 0.2], [0.2, 0.3, 0.6, 0.9, 0.4, 0.8]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(target=t, preds=p)
   >>> metric.compute()
   array([4, 2, 5, 1, 5])















   ..
       !! processed by numpydoc !!
   .. py:method:: update_state(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike) -> None

      
      Update the state variables.
















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> numpy.ndarray

      
      Compute the binary stat scores from the state variables.

      :returns: The binary stat scores. The order is true positives (tp),
                false positives (fp), true negatives (tn), false negatives
                (fn) and support (tp + fn).
      :rtype: numpy.ndarray















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassStatScores(num_classes: int, top_k: Optional[int] = None, classwise: bool = True)

   Bases: :py:obj:`_AbstractScores`

   
   Compute multiclass stat scores.

   :param num_classes: The total number of classes for the problem.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: Optional[int], default=None
   :param classwise: Whether to return the stat scores for each class or sum over all
                     classes.
   :type classwise: bool, default=True

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassStatScores
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 2, 1, 2, 0]
   >>> metric = MulticlassStatScores(num_classes=3, classwise=True)
   >>> metric(target=target, preds=preds)
   array([[1, 1, 3, 0, 1],
           [0, 1, 3, 1, 1],
           [1, 1, 1, 2, 3]])
   >>> metric.reset_state()
   >>> target = [[2, 0, 2, 2, 1], [1, 1, 0, 2, 2]]
   >>> preds = [
   ...         [
   ...             [0.1, 0.2, 0.6],
   ...             [0.6, 0.1, 0.2],
   ...             [0.2, 0.6, 0.1],
   ...             [0.2, 0.6, 0.1],
   ...             [0.6, 0.2, 0.1],
   ...         ],
   ...         [
   ...             [0.05, 0.1, 0.6],
   ...             [0.1, 0.05, 0.6],
   ...             [0.6, 0.1, 0.05],
   ...             [0.1, 0.6, 0.05],
   ...             [0.1, 0.6, 0.05],
   ...         ],
   ...     ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(target=t, preds=p)
   >>> metric.compute()
   array([[2, 1, 7, 0, 2],
           [0, 4, 3, 3, 3],
           [1, 2, 3, 4, 5]])















   ..
       !! processed by numpydoc !!
   .. py:method:: update_state(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike) -> None

      
      Update the state variables.
















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> numpy.ndarray

      
      Compute the multiclass stat scores from the state variables.

      :returns: The multiclass stat scores. The order is true positives (tp),
                false positives (fp), true negatives (tn), false negatives
                (fn) and support (tp + fn). If ``classwise`` is ``True``, the
                shape is ``(num_classes, 5)``. Otherwise, the shape is ``(5,)``.
      :rtype: numpy.ndarray















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelStatScores(num_labels: int, threshold: float = 0.5, top_k: int = None, labelwise: bool = True)

   Bases: :py:obj:`_AbstractScores`

   
   Compute stat scores for multilabel problems.

   :param threshold: Threshold value for binarizing predictions that are probabilities or
                     logits. A sigmoid function is applied if the predictions are logits.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, default=None
   :param labelwise: Whether to return the stat scores for each label or sum over all labels.
   :type labelwise: bool, default=True

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelStatScores
   >>> target = [[0, 1, 1], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.8, 0.2, 0.7]]
   >>> metric = MultilabelStatScores(num_labels=3, labelwise=True)
   >>> metric(target=target, preds=preds)
   array([[1, 0, 1, 0, 1],
           [1, 0, 1, 0, 1],
           [2, 0, 0, 0, 2]])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 1], [1, 0, 1]], [[0, 0, 1], [1, 1, 1]]]
   >>> preds = [[[0.1, 0.9, 0.8], [0.8, 0.2, 0.7]],
   ...         [[0.1, 0.9, 0.8], [0.8, 0.2, 0.7]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(target=t, preds=p)
   >>> metric.compute()
   array([[2, 0, 2, 0, 2],
           [1, 1, 1, 1, 2],
           [4, 0, 0, 0, 4]])















   ..
       !! processed by numpydoc !!
   .. py:method:: update_state(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike) -> None

      
      Update the state variables.
















      ..
          !! processed by numpydoc !!

   .. py:method:: compute() -> numpy.ndarray

      
      Compute the multilabel stat scores from the state variables.

      :returns: The multilabel stat scores. The order is true positives (tp),
                false positives (fp), true negatives (tn), false negatives
                (fn) and support (tp + fn). If ``labelwise`` is ``True``, the
                shape is ``(num_labels, 5)``. Otherwise, the shape is ``(5,)``.
      :rtype: numpy.ndarray















      ..
          !! processed by numpydoc !!


.. py:class:: StatScores

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute stat scores for binary, multiclass and multilabel problems.

   :param task: The task type. Can be either ``binary``, ``multiclass`` or ``multilabel``.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: The positive label to report. Only used for binary tasks.
   :type pos_label: int, default=1
   :param threshold: The threshold to use for binarizing the predictions if logits or
                     probabilities are provided. If logits are provided, a sigmoid function
                     is applied prior to binarization. Used for binary and multilabel tasks.
   :type threshold: float, default=0.5
   :param num_classes: The number of classes for the problem. Required for multiclass tasks.
   :type num_classes: int
   :param classwise: Whether to return the stat scores for each class or sum over all
                     classes. Only used for multiclass tasks.
   :type classwise: bool, default=True
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Used for multiclass and multilabel tasks.
   :type top_k: int, default=None
   :param num_labels: The number of labels. Only used for multilabel tasks.
   :type num_labels: int
   :param labelwise: Whether to compute the stat scores labelwise. Only used for multilabel
                     tasks.
   :type labelwise: bool, default=False

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import StatScores
   >>> target = [0, 1, 1, 0]
   >>> preds = [0, 1, 0, 0]
   >>> metric = StatScores(task="binary", threshold=0.5, pos_label=1)
   >>> metric.update_state(target=target, preds=preds)
   >>> metric.compute()
   array([1, 0, 2, 1, 2])
   >>> metric.reset_state()
   >>> target = [[1, 1, 0, 1, 0, 0], [0, 0, 1, 1, 0, 0]]
   >>> preds = [[0.9, 0.8, 0.3, 0.4, 0.5, 0.2], [0.2, 0.3, 0.6, 0.9, 0.4, 0.8]]
   >>> for t, p in zip(target, preds):
   ...     metric(target=t, preds=p)
   >>> metric.compute()
   array([4, 2, 5, 1, 5])

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import StatScores
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 2, 1, 2, 0]
   >>> metric = StatScores(task="multiclass", num_classes=3, classwise=True)
   >>> metric.update(target=target, preds=preds)
   array([[1, 1, 3, 0, 1],
           [0, 1, 3, 1, 1],
           [1, 1, 1, 2, 3]])
   >>> metric.reset_state()
   >>> target = [[2, 0, 2, 2, 1], [1, 1, 0, 2, 2]]
   >>> preds = [
   ...         [
   ...             [0.1, 0.2, 0.6],
   ...             [0.6, 0.1, 0.2],
   ...             [0.2, 0.6, 0.1],
   ...             [0.2, 0.6, 0.1],
   ...             [0.6, 0.2, 0.1],
   ...         ],
   ...         [
   ...             [0.05, 0.1, 0.6],
   ...             [0.1, 0.05, 0.6],
   ...             [0.6, 0.1, 0.05],
   ...             [0.1, 0.6, 0.05],
   ...             [0.1, 0.6, 0.05],
   ...         ],
   ...     ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(target=t, preds=p)
   >>> metric.compute()
   array([[2, 1, 7, 0, 2],
           [0, 4, 3, 3, 3],
           [1, 2, 3, 4, 5]])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import StatScores
   >>> target = [[0, 1, 1], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.8, 0.2, 0.7]]
   >>> metric = StatScores(task="multilabel", num_labels=3, labelwise=True)
   >>> metric(target=target, preds=preds)
   array([[1, 0, 1, 0, 1],
           [1, 0, 1, 0, 1],
           [2, 0, 0, 0, 2]])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 1], [1, 0, 1]], [[0, 0, 1], [1, 1, 1]]]
   >>> preds = [[[0.1, 0.9, 0.8], [0.8, 0.2, 0.7]],
   ...         [[0.1, 0.9, 0.8], [0.8, 0.2, 0.7]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(target=t, preds=p)
   >>> metric.compute()
   array([[2, 0, 2, 0, 2],
           [1, 1, 1, 1, 2],
           [4, 0, 0, 0, 4]])















   ..
       !! processed by numpydoc !!

