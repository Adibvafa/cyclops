:py:mod:`cyclops.evaluate.metrics.precision_recall`
===================================================

.. py:module:: cyclops.evaluate.metrics.precision_recall

.. autoapi-nested-parse::

   Classes for computing precision and recall metrics.

   ..
       !! processed by numpydoc !!


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.precision_recall.BinaryPrecision
   cyclops.evaluate.metrics.precision_recall.MulticlassPrecision
   cyclops.evaluate.metrics.precision_recall.MultilabelPrecision
   cyclops.evaluate.metrics.precision_recall.Precision
   cyclops.evaluate.metrics.precision_recall.BinaryRecall
   cyclops.evaluate.metrics.precision_recall.MulticlassRecall
   cyclops.evaluate.metrics.precision_recall.MultilabelRecall
   cyclops.evaluate.metrics.precision_recall.Recall




.. py:class:: BinaryPrecision(pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.BinaryStatScores`

   
   Compute the precision score for binary classification tasks.

   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryPrecision
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = BinaryPrecision()
   >>> metric(target, preds)
   0.6666666666666666
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.6666666666666666















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the precision score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassPrecision(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MulticlassStatScores`

   
   Compute the precision score for multiclass classification tasks.

   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each class. Otherwise, use one of the
                   following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false positives.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassPrecision
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> metric = MulticlassPrecision(num_classes=3, average=None)
   >>> metric(target, preds)
   array([1. , 0. , 0.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1., 0., 0.])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the precision score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelPrecision(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MultilabelStatScores`

   
   Compute the precision score for multilabel classification tasks.

   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the precision score for each label. Otherwise,
                   use one of the following options to compute the average precision score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false positives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelPrecision
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.2, 0.8]]
   >>> metric = MultilabelPrecision(num_labels=2, average=None)
   >>> metric(target, preds)
   array([0., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7], [0.2, 0.8]],
   ...     [[0.5, 0.9], [0.3, 0.4]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1., 1.])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the precision score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: Precision

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the precision score for different types of classification tasks.

   This metric can be used for binary, multiclass, and multilabel classification
   tasks. It creates the appropriate metric based on the ``task`` parameter.

   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the precision score for each label/class. Otherwise,
                   use one of the following options to compute the average precision score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and false positives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   (binary)
   >>> from cyclops.evaluation.metrics import Precision
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = Precision(task="binary")
   >>> metric(target, preds)
   0.6666666666666666
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.6666666666666666

   (multiclass)
   >>> from cyclops.evaluation.metrics import Precision
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> metric = Precision(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([1. , 0. , 0.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1., 0., 0.])

   (multilabel)
   >>> from cyclops.evaluation.metrics import Precision
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.2, 0.8]]
   >>> metric = Precision(task="multilabel", num_labels=2)
   >>> metric.update_state(target, preds)
   >>> metric.compute()
   array([0., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7], [0.2, 0.8]],
   ...     [[0.5, 0.9], [0.3, 0.4]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([1., 1.])















   ..
       !! processed by numpydoc !!

.. py:class:: BinaryRecall(pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.BinaryStatScores`

   
   Computes recall score for binary classification.

   :param pos_label: Label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinaryRecall
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 0]
   >>> metric = Recall()
   >>> metric(target, preds)
   0.5
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.5















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the recall score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MulticlassRecall(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MulticlassStatScores`

   
   Compute the recall score for multiclass classification tasks.

   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the recall will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the recall score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false negatives.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassRecall
   >>> target = [0, 1, 2, 0]
   >>> preds = [2, 0, 2, 1]
   >>> metric = MulticlassRecall(num_classes=3)
   >>> metric(target, preds)
   array([0., 0., 1.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.66666667, 0.        , 0.        ])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the recall score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: MultilabelRecall(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.stat_scores.MultilabelStatScores`

   
   Compute the recall score for multilabel classification tasks.

   :param num_labels: Number of labels in the dataset.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param average: If ``None``, return the score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelRecall
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> metric = MultilabelRecall(num_labels=4)
   >>> metric(target, preds)
   array([0., 1., 1. , 0. ])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 0, 1], [0, 0, 1, 1]], [[0, 1, 0, 1], [0, 0, 1, 1]]]
   >>> preds = [[[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]],
   ...          [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0., 1., 1., 0.])















   ..
       !! processed by numpydoc !!
   .. py:method:: compute() -> float

      
      Compute the recall score from the state.
















      ..
          !! processed by numpydoc !!


.. py:class:: Recall

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the recall score for different types of classification tasks.

   This metric can be used for binary, multiclass, and multilabel classification
   tasks. It creates the appropriate class based on the ``task`` parameter.

   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the recall score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import Recall
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = Recall(task="binary")
   >>> metric(target, preds)
   1.
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.5

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import Recall
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> metric = Recall(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([1. , 0. , 0.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.66666667, 0.        , 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import Recall
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.2, 0.8]]
   >>> metric = Recall(task="multilabel", num_labels=2)
   >>> metric(target, preds)
   array([0., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7], [0.2, 0.8]],
   ...     [[0.5, 0.9], [0.3, 0.4]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.33333333, 1.        ])















   ..
       !! processed by numpydoc !!

