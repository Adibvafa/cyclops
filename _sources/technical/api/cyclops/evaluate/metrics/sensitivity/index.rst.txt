:py:mod:`cyclops.evaluate.metrics.sensitivity`
==============================================

.. py:module:: cyclops.evaluate.metrics.sensitivity

.. autoapi-nested-parse::

   Classes for computing sensitivity metrics.

   ..
       !! processed by numpydoc !!


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.sensitivity.BinarySensitivity
   cyclops.evaluate.metrics.sensitivity.MulticlassSensitivity
   cyclops.evaluate.metrics.sensitivity.MultilabelSensitivity
   cyclops.evaluate.metrics.sensitivity.Sensitivity




.. py:class:: BinarySensitivity(pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall.BinaryRecall`

   
   Computes sensitivity score for binary classification.

   :param pos_label: Label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import BinarySensitivity
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 0]
   >>> metric = Sensitivity()
   >>> metric(target, preds)
   0.5
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.5















   ..
       !! processed by numpydoc !!

.. py:class:: MulticlassSensitivity(num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall.MulticlassRecall`

   
   Compute the sensitivity score for multiclass classification tasks.

   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the sensitivity will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the sensitivity score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false negatives.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MulticlassSensitivity
   >>> target = [0, 1, 2, 0]
   >>> preds = [2, 0, 2, 1]
   >>> metric = MulticlassSensitivity(num_classes=3)
   >>> metric(target, preds)
   array([0., 0., 1.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.66666667, 0.        , 0.        ])















   ..
       !! processed by numpydoc !!

.. py:class:: MultilabelSensitivity(num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   Bases: :py:obj:`cyclops.evaluate.metrics.precision_recall.MultilabelRecall`

   
   Compute the sensitivity score for multilabel classification tasks.

   :param num_labels: Number of labels in the dataset.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param average: If ``None``, return the score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics import MultilabelSensitivity
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> metric = MultilabelSensitivity(num_labels=4)
   >>> metric(target, preds)
   array([0., 1., 1. , 0. ])
   >>> metric.reset_state()
   >>> target = [[[0, 1, 0, 1], [0, 0, 1, 1]], [[0, 1, 0, 1], [0, 0, 1, 1]]]
   >>> preds = [[[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]],
   ...          [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0., 1., 1., 0.])















   ..
       !! processed by numpydoc !!

.. py:class:: Sensitivity

   Bases: :py:obj:`cyclops.evaluate.metrics.metric.Metric`

   
   Compute the sensitivity score for different types of classification tasks.

   This metric can be used for binary, multiclass, and multilabel classification
   tasks. It creates the appropriate class based on the ``task`` parameter.

   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the sensitivity score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics import Sensitivity
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> metric = Sensitivity(task="binary")
   >>> metric.update_state(target, preds)
   >>> metric.compute()
   1.
   >>> metric.reset_state()
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   0.5

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics import Sensitivity
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> metric = Sensitivity(task="multiclass", num_classes=3)
   >>> metric(target, preds)
   array([1. , 0. , 0.])
   >>> metric.reset_state()
   >>> target = [[0, 1, 2, 0], [2, 1, 2, 0]]
   >>> preds = [
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]],
   ...     [[0.1, 0.6, 0.3],
   ...      [0.05, 0.1, 0.85],
   ...      [0.2, 0.7, 0.1],
   ...      [0.9, 0.05, 0.05]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.66666667, 0.        , 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics import Sensitivity
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.2, 0.8]]
   >>> metric = Sensitivity(task="multilabel", num_labels=2)
   >>> metric(target, preds)
   array([0., 1.])
   >>> metric.reset_state()
   >>> target = [[[0, 1], [1, 1]], [[1, 1], [1, 0]]]
   >>> preds = [
   ...     [[0.1, 0.7], [0.2, 0.8]],
   ...     [[0.5, 0.9], [0.3, 0.4]]
   ... ]
   >>> for t, p in zip(target, preds):
   ...     metric.update_state(t, p)
   >>> metric.compute()
   array([0.33333333, 1.        ])















   ..
       !! processed by numpydoc !!

