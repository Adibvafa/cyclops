:py:mod:`cyclops.evaluate.metrics.functional.specificity`
=========================================================

.. py:module:: cyclops.evaluate.metrics.functional.specificity

.. autoapi-nested-parse::

   Functions to compute the specificity metric.

   ..
       !! processed by numpydoc !!


Module Contents
---------------

.. py:function:: binary_specificity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn') -> float

   
   Compute specificity for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param pos_label: The label to use for the positive class.
   :type pos_label: int, default=1
   :param threshold: The threshold to use for converting the predictions to binary
                     values. Logits will be converted to probabilities using the sigmoid
                     function.
   :type threshold: float, default=0.5

   :returns: The specificity score.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_specificity
   >>> target = [0, 1, 1, 0, 1]
   >>> preds = [0.1, 0.9, 0.8, 0.5, 0.4]
   >>> binary_specificity(target, preds)
   0.5















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_specificity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute specificity for multiclass classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param num_classes: The number of classes in the dataset.
   :type num_classes: int
   :param top_k: Number of highest probability or logit score predictions considered
                 to find the correct label. Only works when ``preds`` contain
                 probabilities/logits.
   :type top_k: int, optional
   :param average: If None, return the specificity for each class, otherwise return the
                   average specificity. Average options are:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives, false negatives, false positives and true negatives.
                   - ``macro``: Calculate metrics for each class, and find their unweighted
                       mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metrics for each class, and find their
                       average, weighted by support (the number of true instances for each
                       label).
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The specificity score. If ``average`` is None, a numpy.ndarray of
             shape (``num_classes``,) is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_specificity
   >>> target = [0, 1, 2, 0, 1, 2]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]]
   >>> multiclass_specificity(target, preds, num_classes=3)
   array([1.  , 0.75, 1.  ])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_specificity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute specificity for multilabel classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param num_labels: The number of labels in the dataset.
   :type num_labels: int
   :param threshold: The threshold value for converting probability or logit scores to
                     binary. A sigmoid function is first applied to logits to convert them
                     to probabilities.
   :type threshold: float, default=0.5
   :param top_k: Number of highest probability or logit score predictions considered
                 to find the correct label. Only works when ``preds`` contains
                 probabilities/logits.
   :type top_k: int, optional
   :param average: If None, return the specificity for each class, otherwise return the
                   average specificity. Average options are:

                   - ``micro``: Calculate metrics globally by counting the total
                       true positives, false negatives, false positives and true
                       negatives.
                   - ``macro``: Calculate metrics for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their
                       average, weighted by support (the number of true instances for
                       each label).
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The specificity score. If ``average`` is None, a numpy.ndarray of
             shape (``num_labels``,) is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_specificity
   >>> target = [[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 0, 1], [1, 0, 0]]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05]]
   >>> multilabel_specificity(target, preds, num_labels=3)
   array([0.5, 0., 0.5])















   ..
       !! processed by numpydoc !!

.. py:function:: specificity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: int = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: int = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute specificity score for different classification tasks.

   The specificity is the ratio of true negatives to the sum of true negatives and
   false positives. It is also the recall of the negative class.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets as returned by a classifier.
   :type preds: ArrayLike
   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives, false positives, false negatives and true negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: **score** -- The specificity score. If ``average`` is ``None`` and ``task`` is not
             ``binary``, a numpy.ndarray of shape (``num_classes`` or ``num_labels``,)
             is returned.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``task`` is not one of ``binary``, ``multiclass``, or ``multilabel``.

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import specificity
   >>> target = [0, 1, 1, 0, 1]
   >>> preds = [0.9, 0.05, 0.05, 0.35, 0.05]
   >>> specificity(target, preds, task="binary")
   0.5

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import specificity
   >>> target = [0, 1, 2, 0, 1]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05]]
   >>> specificity(target, preds, task="multiclass", num_classes=3)
   array([0.5, 0., 0.5])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import specificity
   >>> target = [[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 0, 1], [1, 0, 0]]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05]]
   >>> specificity(target, preds, task="multilabel", num_labels=3)
   array([0.5, 0., 0.5])















   ..
       !! processed by numpydoc !!

