:py:mod:`cyclops.evaluate.metrics.functional`
=============================================

.. py:module:: cyclops.evaluate.metrics.functional

.. autoapi-nested-parse::

   
   Metrics functional package.
















   ..
       !! processed by numpydoc !!


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   accuracy/index.rst
   auroc/index.rst
   f_beta/index.rst
   precision_recall/index.rst
   precision_recall_curve/index.rst
   roc/index.rst
   sensitivity/index.rst
   specificity/index.rst
   stat_scores/index.rst


Package Contents
----------------


Functions
~~~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.functional.accuracy
   cyclops.evaluate.metrics.functional.binary_accuracy
   cyclops.evaluate.metrics.functional.multiclass_accuracy
   cyclops.evaluate.metrics.functional.multilabel_accuracy
   cyclops.evaluate.metrics.functional.auroc
   cyclops.evaluate.metrics.functional.binary_auroc
   cyclops.evaluate.metrics.functional.multiclass_auroc
   cyclops.evaluate.metrics.functional.multilabel_auroc
   cyclops.evaluate.metrics.functional.binary_f1_score
   cyclops.evaluate.metrics.functional.binary_fbeta_score
   cyclops.evaluate.metrics.functional.f1_score
   cyclops.evaluate.metrics.functional.fbeta_score
   cyclops.evaluate.metrics.functional.multiclass_f1_score
   cyclops.evaluate.metrics.functional.multiclass_fbeta_score
   cyclops.evaluate.metrics.functional.multilabel_f1_score
   cyclops.evaluate.metrics.functional.multilabel_fbeta_score
   cyclops.evaluate.metrics.functional.binary_precision
   cyclops.evaluate.metrics.functional.binary_recall
   cyclops.evaluate.metrics.functional.multiclass_precision
   cyclops.evaluate.metrics.functional.multiclass_recall
   cyclops.evaluate.metrics.functional.multilabel_precision
   cyclops.evaluate.metrics.functional.multilabel_recall
   cyclops.evaluate.metrics.functional.precision
   cyclops.evaluate.metrics.functional.recall
   cyclops.evaluate.metrics.functional.binary_precision_recall_curve
   cyclops.evaluate.metrics.functional.multiclass_precision_recall_curve
   cyclops.evaluate.metrics.functional.multilabel_precision_recall_curve
   cyclops.evaluate.metrics.functional.precision_recall_curve
   cyclops.evaluate.metrics.functional.binary_roc_curve
   cyclops.evaluate.metrics.functional.multiclass_roc_curve
   cyclops.evaluate.metrics.functional.multilabel_roc_curve
   cyclops.evaluate.metrics.functional.roc_curve
   cyclops.evaluate.metrics.functional.binary_sensitivity
   cyclops.evaluate.metrics.functional.multiclass_sensitivity
   cyclops.evaluate.metrics.functional.multilabel_sensitivity
   cyclops.evaluate.metrics.functional.sensitivity
   cyclops.evaluate.metrics.functional.binary_specificity
   cyclops.evaluate.metrics.functional.multiclass_specificity
   cyclops.evaluate.metrics.functional.multilabel_specificity
   cyclops.evaluate.metrics.functional.specificity
   cyclops.evaluate.metrics.functional.binary_stat_scores
   cyclops.evaluate.metrics.functional.multiclass_stat_scores
   cyclops.evaluate.metrics.functional.multilabel_stat_scores
   cyclops.evaluate.metrics.functional.stat_scores



.. py:function:: accuracy(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: int = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: int = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute accuracy score for different classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param task: The type of task for the input data. One of 'binary', 'multiclass'
                or 'multilabel'.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the recall score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives. false positives, true negatives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: **accuracy_score** -- The average accuracy score as a float if ``average`` is not None,
             otherwise a numpy array of accuracy scores per class/label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``task`` is not one of ``binary``, ``multiclass`` or ``multilabel``.
   :raises AssertionError: If ``task`` is ``multiclass`` and ``num_classes`` is not provided or is
       less than 0.
   :raises AssertionError: If ``task`` is ``multilabel`` and ``num_labels`` is not provided or is
       less than 0.

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import accuracy
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> accuracy(target, preds, task="binary")
   0.75

   >>> # (multiclass)
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 0, 2, 2, 1]
   >>> accuracy(target, preds, task="multiclass", num_classes=3, average="micro")
   0.6

   >>> # (multilabel)
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0, 1, 0], [1, 0, 1]]
   >>> accuracy(target, preds, task="multilabel", num_labels=3, average="mcro")
   0.6666666666666666















   ..
       !! processed by numpydoc !!

.. py:function:: binary_accuracy(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn') -> float

   
   Compute accuracy score for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param pos_label: The label of the positive class. Can be 0 or 1.
   :type pos_label: int, default=1
   :param threshold: The threshold value for converting probability or logit scores to
                     binary. A sigmoid function is first applied to logits to convert them
                     to probabilities.
   :type threshold: float, default=0.5
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The accuracy score.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_accuracy
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> binary_accuracy(target, preds)
   0.75
   >>> target = [0, 1, 0, 1]
   >>> preds = [0.1, 0.9, 0.8, 0.4]
   >>> binary_accuracy(target, preds, threshold=0.5)
   0.5















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_accuracy(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the accuracy score for multiclass classification problems.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: Number of highest probability predictions or logits to consider when
                 computing the accuracy score.
   :type top_k: int, default=None
   :param average: If not None, this determines the type of averaging performed on the data:

                   - ``micro``: Calculate metrics globally by counting the total
                       true positives, false negatives, false positives and true
                       negatives.
                   - ``macro``: Calculate metrics for each class, and find their
                       unweighted mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metrics for each class, and find their
                       average, weighted by support (the number of true instances for
                       each class). This alters ``macro`` to account for class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The average accuracy score as a float if ``average`` is not None,
             otherwise a numpy array of accuracy scores per class/label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``average`` is not one of ``micro``, ``macro``, ``weighted`` or ``None``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_accuracy
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 0, 2, 2, 1]
   >>> multiclass_accuracy(target, preds, num_classes=3)
   array([1.        , 0.        , 0.66666667])
   >>> multiclass_accuracy(target, preds, num_classes=3, average="micro")
   0.6
   >>> multiclass_accuracy(target, preds, num_classes=3, average="macro")
   0.5555555555555555
   >>> multiclass_accuracy(target, preds, num_classes=3, average="weighted")
   0.6















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_accuracy(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the accuracy score for multilabel-indicator targets.

   :param target: Ground truth (correct) target values.
   :type target: array-like of shape (num_samples, num_labels)
   :param preds: Estimated targets as returned by a classifier.
   :type preds: array-like of shape (num_samples, num_labels)
   :param num_labels: Number of labels in the multilabel classification task.
   :type num_labels: int
   :param threshold: Threshold value for binarizing the output of the classifier.
   :type threshold: float, default=0.5
   :param top_k: The number of highest probability or logit predictions considered
                 to find the correct label. Only works when ``preds`` contains
                 probabilities/logits.
   :type top_k: int, optional, default=None
   :param average: If None, return the accuracy score per label, otherwise this determines
                   the type of averaging performed on the data:

                   - ``micro``: Calculate metrics globally by counting the total
                       true positives, false negatives, true negatives and false positives.
                   - ``macro``: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their
                       average, weighted by support (the number of true instances for
                       each label).
   :type average: Literal['micro', 'macro', 'weighted', None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal['warn', 0, 1], default="warn"

   :returns: The average accuracy score as a flot if ``average`` is not None,
             otherwise a numpy array of accuracy scores per label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``average`` is not one of ``micro``, ``macro``, ``weighted``,
       or ``None``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_accuracy
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0, 1, 0], [1, 0, 1]]
   >>> multilabel_accuracy(target, preds, num_labels=3, average=None)
   array([1., 1., 0.])
   >>> multilabel_accuracy(target, preds, num_labels=3, average="micro")
   0.6666666666666666















   ..
       !! processed by numpydoc !!

.. py:function:: auroc(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], max_fpr: float = None, thresholds: Union[int, List[float], numpy.ndarray] = None, num_classes: int = None, num_labels: int = None, average: Literal[micro, macro, weighted] = None) -> Union[float, numpy.ndarray]

   
   Compute the area under the ROC curve for different tasks.

   target : ArrayLike
       Ground truth (correct) target values.
   preds : ArrayLike
       Estimated probabilities or decision function. If ``preds`` is not in the
       range [0, 1], a sigmoid function is applied to transform it to the range
       [0, 1].
   task : Literal["binary", "multiclass", "multilabel"]
       Task type. One of ``binary``, ``multiclass``, ``multilabel``.
   max_fpr : float, default=None
       The maximum value of the false positive rate. If not None, a partial AUC
       in the range [0, max_fpr] is returned. Only used for binary classification.
   thresholds : int or list of floats or numpy.ndarray of floats, default=None
       Thresholds used for binarizing the values of ``preds``.
       If int, then the number of thresholds to use.
       If list or array, then the thresholds to use.
       If None, then the thresholds are automatically determined by the unique
       values in ``preds``.
   num_classes : int, default=None
       Number of classes. This parameter is required for the ``multiclass`` task.
   num_labels : int, default=None
       Number of labels. This parameter is required for the ``multilabel`` task.
   average : Literal["micro", "macro", "weighted"], default=None
       If not None, apply the method to compute the average area under the
       ROC curve. Only applicable for the ``multiclass`` and ``multilabel``
       tasks. One of:

       - ``micro``: Calculate metrics globally by counting the total true
           positives, false negatives and false positives.
       - ``macro``: Calculate metrics for each label, and find their unweighted
           mean. This does not take label imbalance into account.
       - ``weighted``: Calculate metrics for each label, and find their
           average, weighted by support (accounting for label imbalance).

   :returns: **auroc_score** -- Area under the ROC curve. If ``average`` is None or task is ``binary``,
             ``auroc`` is a float. Otherwise, ``auroc`` is a numpy array.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import auroc
   >>> target = [0, 1, 0, 1]
   >>> preds = [0.1, 0.35, 0.4, 0.8]
   >>> auroc(target, preds, task="binary")
   0.75

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import auroc
   >>> target = [0, 1, 2, 0, 1, 2]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.5, 0.3, 0.2],
   ...          [0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.5, 0.3, 0.2]]
   >>> auroc(target, preds, task="multiclass", num_classes=3, average=None)
   array([0.5, 1. , 0.5])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import auroc
   >>> target = [[0, 1], [1, 1], [0, 0], [1, 0]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2], [0.4, 0.6], [0.2, 0.8]]
   >>> auroc(target, preds, task="multilabel", num_labels=2, average=None)
   array([0.25, 0.5 ])















   ..
       !! processed by numpydoc !!

.. py:function:: binary_auroc(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, max_fpr: float = None, thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1) -> float

   
   Compute the area under the ROC curve for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If the values in ``preds``
                 are not in the range [0, 1], then they will be transformed to this range
                 via a sigmoid function.
   :type preds: ArrayLike
   :param max_fpr: The maximum value of the false positive rate. If not None, then
                   the partial AUCROC in the range [0, max_fpr] is returned.
   :type max_fpr: float, default=None
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None
   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1

   :returns: **auroc** -- Area under the ROC curve.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_auroc
   >>> target = [1, 0, 0, 1]
   >>> preds = [0.1, 0.9, 0.4, 0.6]
   >>> binary_auroc(target, preds, thresholds=5)
   0.25















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_auroc(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, thresholds: Union[int, List[float], numpy.ndarray] = None, average: Literal[macro, weighted] = None) -> Union[float, numpy.ndarray]

   
   Compute the area under the ROC curve for multiclass classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If the values in ``preds``
                 are not in the range [0, 1], then they will be transformed to this range
                 via a softmax function.
   :type preds: ArrayLike
   :param num_classes: Number of classes.
   :type num_classes: int
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None
   :param average: If ``None``, then the scores for each class are returned. Otherwise,
                   this determines the type of averaging performed on the scores. One of

                   - `macro`: Calculate metrics for each class, and find their unweighted
                       mean. This does not take class imbalance into account.
                   - `weighted`: Calculate metrics for each class, and find their average,
                       weighted by support (the number of true instances for each class).
   :type average: Literal["macro", "weighted"], default=None

   :returns: **auroc** -- Area under the ROC curve. If ``average`` is ``None``, then a numpy array
             of shape (num_classes,) is returned, otherwise a float is returned.
   :rtype: Union[float, numpy.ndarray]

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_auroc
   >>> target = [1, 0, 2, 0]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05],
   ...         [0.05, 0.05, 0.9], [0.9, 0.05, 0.05]]
   >>> multiclass_auroc(target, preds, num_classes=3, thresholds=5,
   ...     average=None
   ... )
   array([0.5       , 0.33333333, 1.        ])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_auroc(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, thresholds: Union[int, List[float], numpy.ndarray] = None, average: Literal[micro, macro, weighted] = None) -> Union[float, numpy.ndarray]

   
   Compute the area under the ROC curve for multilabel classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If the values in ``preds``
                 are not in the range [0, 1], then they will be transformed to this range
                 via a softmax function.
   :type preds: ArrayLike
   :param num_labels: Number of labels.
   :type num_labels: int
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None
   :param average: If ``None``, then the scores for each label are returned. Otherwise,
                   this determines the type of averaging performed on the scores. One of

                   - `micro`: Calculate metrics globally by counting the total true
                       positives, false negatives and false positives.
                   - `macro`: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - `weighted``: Calculate metrics for each label, and find their average,
                       weighted by support (the number of true instances for each label).
   :type average: Literal["micro", "macro", "weighted"], default=None

   :returns: Area under the ROC curve. If ``average`` is ``None``, then a numpy array
             of shape (num_labels,) is returned, otherwise a float is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_auroc
   >>> target = [[0, 1, 0], [0, 1, 1], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.9], [0.8, 0.2, 0.3]]
   >>> multilabel_auroc(target, preds, num_labels=3, thresholds=5,
   ...     average=None)
   array([1.  , 0.75, 0.25])















   ..
       !! processed by numpydoc !!

.. py:function:: binary_f1_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn') -> float

   
   Compute the F1 score for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold value for binarizing predictions in form of logits or
                     probability scores.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The F1 score.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_f1_score
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.8, 0.4, 0.3]
   >>> binary_f1_score(target, preds)
   0.6666666666666666















   ..
       !! processed by numpydoc !!

.. py:function:: binary_fbeta_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, beta: float, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn') -> float

   
   Compute the F-beta score for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param pos_label: The positive class label. One of [0, 1].
   :type pos_label: int, default=1
   :param threshold: Threshold value for converting probabilities and logits to binary.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there are no true positives or true negatives.
                         If set to ``warn``, this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The binary F-beta score.
   :rtype: float

   :raises ValueError: beta is less than 0.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_fbeta_score
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> binary_fbeta_score(target, preds, beta=0.5)
   0.7142857142857143















   ..
       !! processed by numpydoc !!

.. py:function:: f1_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: int = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: int = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the F1 score for multiclass data.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets as returned by a classifier.
   :type preds: ArrayLike
   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The F1 score. If ``average`` is ``None`` and ``task`` is not ``binary``,
             a numpy.ndarray of shape (``num_classes`` or ``num_labels``,) is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import f1_score
   >>> target = [0, 1, 0, 1]
   >>> preds = [0.1, 0.9, 0.8, 0.2]
   >>> f1_score(target, preds, task="binary")
   0.5

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import f1_score
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.2, 0.2, 0.6], [0.9, 0.1, 0]]
   >>> f1_score(target, preds, task="multiclass", num_classes=3)
   array([0.66666667, 0.8       , 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import f1_score
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.2]]
   >>> f1_score(target, preds, task="multilabel", num_labels=3)
   array([0., 1., 1.])















   ..
       !! processed by numpydoc !!

.. py:function:: fbeta_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, beta: float, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: int = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: int = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the F-beta score for binary, multiclass, or multilabel data.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets as returned by a classifier.
   :type preds: ArrayLike
   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: **score** -- The F-beta score. If ``average`` is not ``None`` and ``task`` is not
             ``binary``, a numpy array of shape (num_classes,) is returned.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``task`` is not one of ``binary``, ``multiclass``, or
       ``multilabel``.

   .. rubric:: Examples

   (binary)
   >>> from cyclops.evaluation.metrics.functional import fbeta_score
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.8, 0.4, 0.3]
   >>> fbeta_score(target, preds, beta=0.5, task="binary")
   0.8333333333333334

   (multiclass)
   >>> from cyclops.evaluation.metrics.functional import fbeta_score
   >>> target = [0, 1, 2, 2]
   >>> preds = [1 2, 2, 0]
   >>> fbeta_score(target, preds, beta=0.5, task="multiclass", num_classes=3)
   array([0.83333333, 0.        , 0.55555556])

   (multilabel)
   >>> from cyclops.evaluation.metrics.functional import fbeta_score
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> fbeta_score(target, preds, beta=0.5, task="multilabel", num_labels=2)
   array([1.        , 0.83333333])















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_f1_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the F1 score for multiclass classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each class. Otherwise, use one of
                   the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class
                       imbalance. It can result in an F-score that is not between
                       precision and recall.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The F1 score. If ``average`` is ``None``, a numpy.ndarray of shape
             (``num_classes``,) is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_f1_score
   >>> target = [0, 1, 2, 0]
   >>> preds = [1, 1, 1, 0]
   >>> multiclass_f1_score(target, preds, num_classes=3)
   array([0.66666667, 0.5       , 0.        ])















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_fbeta_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, beta: float, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the F-beta score for multiclass data.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param num_classes: The number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The multiclass F-beta score. If ``average`` is ``None``, a numpy array
             of shape (num_classes,) is returned.
   :rtype: float or numpy.ndarray

   :raises ValueError: ``average`` is not one of ``micro``, ``macro``, ``weighted``, or ``None``,
       or ``beta`` is less than 0.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_fbeta_score
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> multiclass_fbeta_score(target, preds, beta=0.5, num_classes=3)
   array([1., 0., 0.])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_f1_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   
   Compute the F1 score for multilabel classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each label. Otherwise, use one of
                   the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The F1 score. If ``average`` is ``None``, a numpy.ndarray of shape
             (``num_labels``,) is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_f1_score
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.2]]
   >>> multilabel_f1_score(target, preds, num_labels=3)
   array([0., 1., 1.])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_fbeta_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, beta: float, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   
   Compute the F-beta score for multilabel data.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each label. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The multilabel F-beta score. If ``average`` is ``None``, a numpy array
             of shape (num_labels,) is returned.
   :rtype: float or numpy.ndarray

   :raises ValueError: ``average`` is not one of ``micro``, ``macro``, ``weighted``, or ``None``,
       or ``beta`` is less than 0.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_fbeta_score
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> multilabel_fbeta_score(target, preds, beta=0.5, num_labels=2)
   array([1.        , 0.83333333])















   ..
       !! processed by numpydoc !!

.. py:function:: binary_precision(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn') -> float

   
   Compute precision score for binary classification.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: Precision score.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_precision
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> binary_precision(target, preds)
   0.6666666666666666
   >>> target = [[0, 1, 0, 1], [0, 0, 1, 1]]
   >>> preds = [[0.1, 0.9, 0.8, 0.2], [0.2, 0.3, 0.6, 0.1]]
   >>> binary_precision(target, preds)
   0.6666666666666666















   ..
       !! processed by numpydoc !!

.. py:function:: binary_recall(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn')

   
   Compute recall score for binary classification.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param pos_label: Label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: Recall score.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_recall
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 0]
   >>> binary_recall(target, preds)
   0.5















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_precision(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute precision score for multiclass classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the precision score for each class. Otherwise,
                   use one of the following options to compute the average precision score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false positives.
                   - ``macro``: Calculate metric for each class, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their average
                       weighted by the support (the number of true instances for each class).
                       This alters "macro" to account for class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: **precision** -- Precision score. If ``average`` is None, return a numpy.ndarray of
             precision scores for each class.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``average`` is not one of ``micro``, ``macro``, ``weighted``
       or ``None``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_precision
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> multiclass_precision(target, preds, num_classes=3)
   array([1., 0., 0.])















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_recall(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   
   Compute recall score for multiclass classification.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param num_classes: Number of classes.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the recall will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: Optional[int]
   :param average: Average to apply. If None, return scores for each class. Default is
                   None. One of:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and false negatives.
                   - ``macro``: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their
                       average weighted by support (the number of true instances for each label).
                       This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None]
   :param zero_division: Value to return when there are no true positives or true negatives.
                         If set to ``warn``, this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1]

   :returns: Recall score. If ``average`` is None, return a numpy.ndarray of
             recall scores for each class.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``average`` is not one of ``micro``, ``macro``, ``weighted``
       or ``None``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_recall
   >>> target = [0, 1, 2, 0, 1, 2]
   >>> preds = [[0.4, 0.1, 0.5], [0.1, 0.8, 0.1], [0.2, 0.2, 0.6],
   ...     [0.5, 0.3, 0.2], [0.2, 0.5, 0.3], [0.2, 0.2, 0.6]]
   >>> multiclass_recall(target, preds, num_classes=3, average="macro")
   0.8333333333333334















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_precision(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   
   Compute precision score for multilabel classification tasks.

   The input is expected to be an array-like of shape (N, L), where N is the
   number of samples and L is the number of labels. The input is expected to
   be a binary array-like, where 1 indicates the presence of a label and 0
   indicates its absence.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param average: If ``None``, return the precision score for each label. Otherwise,
                   use one of the following options to compute the average precision score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false positives.
                   - ``macro``: Calculate metric for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their average
                       weighted by the support (the number of true instances for each label).
                       This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: **precision** -- Precision score. If ``average`` is None, return a numpy.ndarray of
             precision scores for each label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If average is not one of ``micro``, ``macro``, ``weighted``,
       or ``None``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_precision
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.2, 0.8]]
   >>> multilabel_precision(target, preds, num_labels=2)
   array([0., 1. ])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_recall(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   
   Compute recall score for multilabel classification tasks.

   The input is expected to be an array-like of shape (N, L), where N is the
   number of samples and L is the number of labels. The input is expected to
   be a binary array-like, where 1 indicates the presence of a label and 0
   indicates its absence.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param num_labels: Number of labels in the dataset.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param average: If ``None``, return the recall score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: Recall score. If ``average`` is None, return a numpy.ndarray of
             recall scores for each label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``average`` is not one of ``micro``, ``macro``, ``weighted``
       or ``None``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_recall
   >>> target = [1, 1, 2, 0, 2, 2]
   >>> preds = [1, 2, 2, 0, 2, 0]
   >>> multilabel_recall(target, preds, num_classes=3)
   array([1.        , 0.5       , 0.66666667])















   ..
       !! processed by numpydoc !!

.. py:function:: precision(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: Optional[int] = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute precision score for different classification tasks.

   Precision is the ratio of correctly predicted positive observations to the
   total predicted positive observations.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param task: Task type.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label of the positive class. Only used for binary classification.
   :type pos_label: int
   :param num_classes: Number of classes. Only used for multiclass classification.
   :type num_classes: Optional[int]
   :param threshold: Threshold for positive class predictions. Default is 0.5.
   :type threshold: float
   :param top_k: Number of highest probability or logits predictions to consider when
                 computing multiclass or multilabel metrics. Default is None.
   :type top_k: Optional[int]
   :param num_labels: Number of labels. Only used for multilabel classification.
   :type num_labels: Optional[int]
   :param average: Average to apply. If None, return scores for each class. Default is
                   None. One of:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and and false positives.
                   - ``macro``: Calculate metrics for each label/class, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their
                       average weighted by support (the number of true instances for
                       each label). This alters ``macro`` to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None]
   :param zero_division: Value to return when there are no true positives or true negatives.
                         If set to ``warn``, this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1]

   :returns: **precision_score** -- Precision score. If ``average`` is not None or task is ``binary``,
             return a float. Otherwise, return a numpy.ndarray of precision scores
             for each class/label.
   :rtype: numpy.ndarray or float

   :raises ValueError: If task is not one of ``binary``, ``multiclass`` or ``multilabel``.

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import precision
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.9, 0.8, 0.3]
   >>> precision(target, preds, task="binary")
   1.

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import precision
   >>> target = [0, 1, 2, 0, 1, 2]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.1, 0.8, 0.1],
   ...         [0.5, 0.3, 0.2],  [0.2, 0.5, 0.3], [0.2, 0.2, 0.6]]
   >>> precision(target, preds, task="multiclass", num_classes=3,
   ...     average="macro")
   0.8333333333333334

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import precision
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.2, 0.8]]
   >>> precision(target, preds, task="multilabel", num_labels=2,
   ...     average="macro")
   0.5















   ..
       !! processed by numpydoc !!

.. py:function:: recall(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: int = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: int = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute recall score for different classification tasks.

   Recall is the ratio tp / (tp + fn) where tp is the number of true positives
   and fn the number of false negatives. The recall is intuitively the ability
   of the classifier to find all the positive samples.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param task: Task type.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label of the positive class. Only used for binary classification.
   :type pos_label: int
   :param num_classes: Number of classes. Only used for multiclass classification.
   :type num_classes: Optional[int]
   :param threshold: Threshold for positive class predictions.
   :type threshold: float, default=0.5
   :param top_k: Number of highest probability or logits predictions to consider when
                 computing multiclass or multilabel metrics. Default is None.
   :type top_k: Optional[int]
   :param num_labels: Number of labels. Only used for multilabel classification.
   :type num_labels: Optional[int]
   :param average: Average to apply. If None, return scores for each class. One of:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and false negatives.
                   - ``macro``: Calculate metrics for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their
                       average weighted by support (the number of true instances for
                       each label). This alters ``macro`` to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there are no true positives or true negatives.
                         If set to ``warn``, this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1]

   :returns: **recall_score** -- Recall score. If ``average`` is not None or ``task`` is ``binary``,
             return a float. Otherwise, return a numpy.ndarray of recall scores
             for each class/label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``task`` is not one of ``binary``, ``multiclass`` or ``multilabel``.

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import recall
   >>> target = [0, 1, 1, 0, 1]
   >>> preds = [0.4, 0.2, 0.0, 0.6, 0.9]
   >>> recall(target, preds, task="binary")
   0.3333333333333333

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import recall
   >>> target = [1, 1, 2, 0, 2, 2]
   >>> preds = [1, 2, 2, 0, 2, 0]
   >>> recall(target, preds, task="multiclass", num_classes=3)
   array([1.        , 0.5       , 0.66666667])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import recall
   >>> target = [[1, 0, 1], [0, 1, 0]]
   >>> preds = [[0.4, 0.2, 0.0], [0.6, 0.9, 0.1]]
   >>> recall(target, preds, task="multilabel", num_labels=3)
   array([0., 1., 0.])















   ..
       !! processed by numpydoc !!

.. py:function:: binary_precision_recall_curve(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1) -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]

   
   Compute precision-recall curve for binary input.

   :param target: Binary target values.
   :type target: ArrayLike
   :param preds: Predicted probabilities or output of a decision function. If ``preds``
                 are logits, they will be transformed to probabilities via the sigmoid
                 function.
   :type preds: ArrayLike
   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list or numpy.ndarray, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param pos_label: The label of the positive class.
   :type pos_label: int

   :returns: * **precision** (*numpy.ndarray*) -- Precision scores such that element i is the precision of predictions
               with score >= thresholds[i].
             * **recall** (*numpy.ndarray*) -- Recall scores in descending order.
             * **thresholds** (*numpy.ndarray*) -- Thresholds used for computing the precision and recall scores.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import (
   ...     binary_precision_recall_curve
   ... )
   >>> target = [0, 0, 1, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> precision, recall, thresholds = binary_precision_recall_curve(target,
   ...     preds, thresholds=5
   ... )
   >>> precision
   array([0.5, 0.66666667, 1., 1., 0.]
   >>> recall
   array([1., 1., 0.5, 0.5, 0.])
   >>> thresholds
   array([0.1, 0.25 , 0.5, 0.75 , 1.])















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_precision_recall_curve(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, thresholds: Union[int, List[float], numpy.ndarray] = None) -> Union[Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray], Tuple[List[numpy.ndarray], List[numpy.ndarray], List[numpy.ndarray]]]

   
   Compute the precision-recall curve for multiclass problems.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If ``preds`` is a logit, it
                 will be converted to a probability using the softmax function.
   :type preds: ArrayLike
   :param num_classes: The number of classes in the dataset.
   :type num_classes: int
   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None

   :returns: * **precision** (*numpy.ndarray or list of numpy.ndarray*) -- Precision scores where element i is the precision score corresponding
               to the threshold i. If state is a tuple of the target and predicted
               probabilities, then precision is a list of arrays, where each array
               corresponds to the precision scores for a class.
             * **recall** (*numpy.ndarray or list of numpy.ndarray*) -- Recall scores where element i is the recall score corresponding to
               the threshold i. If state is a tuple of the target and predicted
               probabilities, then recall is a list of arrays, where each array
               corresponds to the recall scores for a class.
             * **thresholds** (*numpy.ndarray or list of numpy.ndarray*) -- Thresholds used for computing the precision and recall scores.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import (
   ...     multiclass_precision_recall_curve
   ... )
   >>> target = [0, 1, 2, 2]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.5, 0.3, 0.2], [0.3, 0.4, 0.3]]
   >>> precision, recall, thresholds = multiclass_precision_recall_curve(target,
   ...     preds, num_classes=3, thresholds=5)
   >>> precision
   array([[0.25, 0.  , 0.  , 0.  , 0.  , 1.  ],
   [0.25, 0.25, 0.5 , 1.  , 0.  , 1.  ],
   [0.5 , 0.5 , 0.  , 0.  , 0.  , 1.  ]])
   >>> recall
   array([[1. , 0. , 0. , 0. , 0. , 0. ],
   [1. , 1. , 1. , 1. , 0. , 0. ],
   [1. , 0.5, 0. , 0. , 0. , 0. ]])
   >>> thresholds
   array([0.  , 0.25, 0.5 , 0.75, 1.  ])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_precision_recall_curve(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, thresholds: Union[int, List[float], numpy.ndarray] = None) -> Union[Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray], Tuple[List[numpy.ndarray], List[numpy.ndarray], List[numpy.ndarray]]]

   
   Compute the precision-recall curve for multilabel input.

   :param target: The target values.
   :type target: ArrayLike
   :param preds: Predicted probabilities or output of a decision function. If the
                 values are not in [0, 1], then they are converted into that range
                 by applying the sigmoid function.
   :type preds: ArrayLike
   :param num_labels: The number of labels in the dataset.
   :type num_labels: int
   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list of floats, then the thresholds to use.
                      If None, then the thresholds are computed automatically from the unique
                      values in ``preds``.
   :type thresholds: numpy.ndarray

   :returns: * **precision** (*numpy.ndarray or List[numpy.ndarray]*) -- Precision values for each label. If ``thresholds`` is None, then
               precision is a list of arrays, one for each label. Otherwise,
               precision is a single array with shape
               (``num_labels``, len(``thresholds``)).
             * **recall** (*numpy.ndarray or List[numpy.ndarray]*) -- Recall values for each label. If ``thresholds`` is None, then
               recall is a list of arrays, one for each label. Otherwise,
               recall is a single array with shape (``num_labels``, len(``thresholds``)).
             * **thresholds** (*numpy.ndarray or List[numpy.ndarray]*) -- If ``thresholds`` is None, then thresholds is a list of arrays, one for
               each label. Otherwise, thresholds is a single array with shape
               (len(``thresholds``,).

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import (
   ...     multilabel_precision_recall_curve)
   >>> target = [[1, 1, 0], [0, 1, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.95, 0.35]]
   >>> precision, recall, thresholds = multilabel_precision_recall_curve(
   ...     target, preds, num_labels=3, thresholds=5)
   >>> precision
   array([[0.5, 0. , 0. , 0. , 0. , 1. ],
   [1. , 1. , 1. , 1. , 0. , 1. ],
   [0. , 0. , 0. , 0. , 0. , 1. ]])
   >>> recall
   array([[1., 0., 0., 0., 0., 0.],
   [1., 1., 1., 1., 0., 0.],
   [0., 0., 0., 0., 0., 0.]])
   >>> thresholds
   array([0.  , 0.25, 0.5 , 0.75, 1.  ])















   ..
       !! processed by numpydoc !!

.. py:function:: precision_recall_curve(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1, num_classes: int = None, num_labels: int = None) -> Union[Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray], Tuple[List[numpy.ndarray], List[numpy.ndarray], List[numpy.ndarray]]]

   
   Compute the precision-recall curve for different tasks/input types.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or non-thresholded output of decision function.
   :type preds: ArrayLike
   :param task: The task for which the precision-recall curve is computed.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param thresholds: Thresholds used for computing the precision and recall scores. If int,
                      then the number of thresholds to use. If list or array, then the
                      thresholds to use. If None, then the thresholds are automatically
                      determined by the sunique values in ``preds``
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1
   :param num_classes: The number of classes in the dataset. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, optional
   :param num_labels: The number of labels in the dataset. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, optional

   :returns: * **precision** (*numpy.ndarray*) -- The precision scores where ``precision[i]`` is the precision score for
               ``scores >= thresholds[i]``. If ``task`` is 'multiclass' or 'multilaabel',
               then ``precision`` is a list of numpy arrays, where ``precision[i]`` is the
               precision scores for class or label ``i``.
             * **recall** (*numpy.ndarray*) -- The recall scores where ``recall[i]`` is the recall score for ``scores >=
               thresholds[i]``. If ``task`` is 'multiclass' or 'multilaabel', then
               ``recall`` is a list of numpy arrays, where ``recall[i]`` is the recall
               scores for class or label ``i``.
             * **thresholds** (*numpy.ndarray*) -- Thresholds used for computing the precision and recall scores.

   :raises ValueError: If ``task`` is not one of 'binary', 'multiclass' or 'multilabel'.
   :raises AssertionError: If ``task`` is ``multiclass`` and ``num_classes`` is not provided.
   :raises AssertionError: If ``task`` is ``multilabel`` and ``num_labels`` is not provided.

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import precision_recall_curve
   >>> target = [0, 0, 1, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> precision, recall, thresholds = precision_recall_curve(target, preds,
   ...     "binary")
   >>> precision
   array([0.66666667, 0.5, 1., 1.])
   >>> recall
   array([1. , 0.5, 0.5, 0. ])
   >>> thresholds
   array([0.35, 0.4 , 0.8 ])

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import precision_recall_curve
   >>> target = [0, 1, 2, 2]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.5, 0.3, 0.2], [0.3, 0.4, 0.3]]
   >>> precision, recall, thresholds = precision_recall_curve(
   ...     target, preds, task="multiclass", num_classes=3)
   >>> precision
   [array([0.33333333, 0.        , 0.        , 1.        ]),
   array([1., 1.]),
   array([0.66666667, 0.5       , 1.        ])]
   >>> recall
   [array([1., 0., 0., 0.]), array([1., 0.]), array([1. , 0.5, 0. ])]
   >>> thresholds
   [array([0.1, 0.3, 0.5]), array([0.95]), array([0.2, 0.3])]

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import precision_recall_curve
   >>> target = [[1, 1, 0], [0, 1, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.95, 0.35]]
   >>> precision, recall, thresholds = precision_recall_curve(target, preds,
   ...     "multilabel", num_labels=3)
   >>> precision
   [array([1., 1.]), array([1., 1., 1.]), array([0., 1.])]
   >>> recall
   [array([1., 0.]), array([1. , 0.5, 0. ]), array([0., 0.])]
   >>> thresholds
   [array([0.1]), array([0.9 , 0.95]), array([0.8])]















   ..
       !! processed by numpydoc !!

.. py:function:: binary_roc_curve(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1) -> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]

   
   Compute the ROC curve for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If ``preds`` is not in
                 the range [0, 1], a sigmoid function is applied to transform it to
                 the range [0, 1].
   :type preds: ArrayLike
   :param thresholds: Thresholds used for computing the precision and recall scores.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param pos_label: The label of the positive class.
   :type pos_label: int, optional

   :returns: * **fpr** (*numpy.ndarray*) -- False positive rate.
             * **tpr** (*numpy.ndarray*) -- True positive rate.
             * **thresholds** (*numpy.ndarray*) -- Thresholds used to compute fpr and tpr.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_roc_curve
   >>> target = [1, 0, 1, 0]
   >>> preds = [0.9, 0.2, 0.8, 0.3]
   >>> fpr, tpr, thresholds = binary_roc_curve(target, preds, thresholds=5)
   >>> fpr
   array([0. , 0. , 0. , 0.5, 1. ])
   >>> tpr
   array([0., 1., 1., 1., 1.])
   >>> thresholds
   array([1.  , 0.75, 0.5 , 0.25, 0.  ])















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_roc_curve(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, thresholds: Union[int, List[float], numpy.ndarray] = None) -> Union[Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray], Tuple[List[numpy.ndarray], List[numpy.ndarray], List[numpy.ndarray]]]

   
   Compute the ROC curve for multiclass classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If ``preds`` is not in
                 the range [0, 1], a softmax function is applied to transform it to
                 the range [0, 1].
   :type preds: ArrayLike
   :param num_classes: Number of classes.
   :type num_classes: int
   :param thresholds: Thresholds used for binarizing the predicted probabilities.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None

   :returns: * **fpr** (*numpy.ndarray or list of numpy.ndarray*) -- False positive rate. If ``threshold`` is not None, ``fpr`` is a 1d numpy
               array. Otherwise, ``fpr`` is a list of 1d numpy arrays, one for each
               class.
             * **tpr** (*numpy.ndarray or list of numpy.ndarray*) -- True positive rate. If ``threshold`` is not None, ``tpr`` is a 1d numpy
               array. Otherwise, ``tpr`` is a list of 1d numpy arrays, one for each class.
             * **thresholds** (*numpy.ndarray or list of numpy.ndarray*) -- Thresholds used to compute fpr and tpr. ``threshold`` is not None,
               thresholds is a 1d numpy array. Otherwise, thresholds is a list of
               1d numpy arrays, one for each class.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_roc_curve
   >>> target = [1, 0, 2, 0]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05],
   ...         [0.05, 0.05, 0.9], [0.9, 0.05, 0.05]]
   >>> fpr, tpr, thresholds = multiclass_roc_curve(target, preds,
   ...     num_classes=3, thresholds=5
   ... )
   >>> fpr
   array([[0.        , 0.5       , 0.5       , 0.5       , 1.        ],
   [0.        , 0.33333333, 0.33333333, 0.33333333, 1.        ],
   [0.        , 0.        , 0.        , 0.        , 1.        ]])
   >>> tpr
   array([[0. , 0.5, 0.5, 0.5, 1. ],
   [0. , 0. , 0. , 0. , 1. ],
   [0. , 1. , 1. , 1. , 1. ]])
   >>> thresholds
   array([1.  , 0.75, 0.5 , 0.25, 0.  ])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_roc_curve(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, thresholds: Union[int, List[float], numpy.ndarray] = None) -> Union[Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray], Tuple[List[numpy.ndarray], List[numpy.ndarray], List[numpy.ndarray]]]

   
   Compute the ROC curve for multilabel classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If ``preds`` is not in
                 the range [0, 1], a sigmoid function is applied to transform it to
                 the range [0, 1].
   :type preds: ArrayLike
   :param num_labels: The number of labels in the dataset.
   :type num_labels: int
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None

   :returns: * **fpr** (*numpy.ndarray or list of numpy.ndarray*) -- False positive rate. If ``threshold`` is not None, ``fpr`` is a 1d numpy
               array. Otherwise, ``fpr`` is a list of 1d numpy arrays, one for each
               label.
             * **tpr** (*numpy.ndarray or list of numpy.ndarray*) -- True positive rate. If ``threshold`` is not None, ``tpr`` is a 1d numpy
               array. Otherwise, ``tpr`` is a list of 1d numpy arrays, one for each label.
             * **thresholds** (*numpy.ndarray or list of numpy.ndarray*) -- Thresholds used to compute fpr and tpr. ``threshold`` is not None,
               thresholds is a 1d numpy array. Otherwise, thresholds is a list of
               1d numpy arrays, one for each label.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_roc_curve
   >>> target = [[0, 1, 0], [0, 1, 1], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.9], [0.8, 0.2, 0.3]]
   >>> fpr, tpr, thresholds = multilabel_roc_curve(target, preds, num_labels=3,
   ...     thresholds=5
   ... )
   >>> fpr
   array([[0., 0., 0., 0., 1.],
   [0., 0., 0., 0., 1.],
   [0., 1., 1., 1., 1.]])
   >>> tpr
   array([[0. , 1. , 1. , 1. , 1. ],
   [0. , 0.5, 0.5, 0.5, 1. ],
   [0. , 0.5, 0.5, 1. , 1. ]])
   >>> thresholds
   array([1.  , 0.75, 0.5 , 0.25, 0.  ])















   ..
       !! processed by numpydoc !!

.. py:function:: roc_curve(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1, num_classes: int = None, num_labels: int = None) -> Union[Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray], Tuple[List[numpy.ndarray], List[numpy.ndarray], List[numpy.ndarray]]]

   
   Compute the ROC curve for different tasks/input types.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or non-thresholded output of decision function.
                 If ``task`` is ``multiclass`` and the values in ``preds`` are not
                 probabilities, they will be converted to probabilities using the softmax
                 function. If ``task`` is ``multilabel`` and the values in ``preds`` are
                 not probabilities, they will be converted to probabilities using the
                 sigmoid function.
   :type preds: ArrayLike
   :param task: The type of task for the input data. One of 'binary', 'multiclass'
                or 'multilabel'.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param thresholds: Thresholds used for computing the ROC curve. Can be one of:

                      - None: use the unique values of ``preds`` as thresholds
                      - int: generate ``thresholds`` number of evenly spaced values between
                          0 and 1 as thresholds.
                      - list of floats: use the values in the list as thresholds. The list
                          of values should be monotonically increasing. The list will be
                          converted into a numpy array.
                      - numpy.ndarray of floats: use the values in the array as thresholds.
                          The array should be 1d and monotonically increasing.
   :type thresholds: int or list of floats or numpy.ndarray of floats, default=None
   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1
   :param num_classes: The number of classes in the dataset. Required for multiclass tasks.
   :type num_classes: int, optional
   :param num_labels: The number of labels in the dataset. Required for multilabel tasks.
   :type num_labels: int, optional

   :returns: * **fpr** (*numpy.ndarray or list of numpy.ndarray*) -- False positive rate. If ``task`` is 'binary' or ``threshold`` is not None,
               ``fpr`` is a 1d numpy array. If ``task`` is 'multiclass' or 'multilabel',
               and ``threshold`` is None, then ``fpr`` is a list of 1d numpy
               arrays, one for each class or label.
             * **tpr** (*numpy.ndarray or list of numpy.ndarray*) -- True positive rate. If ``task`` is 'binary' or ``threshold`` is not None,
               ``tpr`` is a 1d numpy array. If ``task`` is 'multiclass' or 'multilabel',
               and ``threshold`` is None, then ``tpr`` is a list of 1d numpy
               arrays, one for each class or label.
             * **thresholds** (*numpy.ndarray or list of numpy.ndarray*) -- Thresholds used to compute fpr and tpr. If ``task`` is 'binary' or
               ``threshold`` is not None, ``thresholds`` is a 1d numpy array. If
               ``task`` is 'multiclass' or 'multilabel', and ``threshold`` is None,
               then ``thresholds`` is a list of 1d numpy arrays, one for each class
               or label.

   :raises ValueError: If ``task`` is not one of 'binary', 'multiclass' or 'multilabel'.
   :raises AssertionError: If ``task`` is ``multiclass`` and ``num_classes`` is not provided.
   :raises AssertionError: If ``task`` is ``multilabel`` and ``num_labels`` is not provided.

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import roc_curve
   >>> target = [0, 0, 1, 1]
   >>> preds = [0.1, 0.4, 0.35, 0.8]
   >>> fpr, tpr, thresholds = roc_curve(target, preds, task='binary')
   >>> fpr
   array([0. , 0. , 0.5, 0.5, 1. ])
   >>> tpr
   array([0. , 0.5, 0.5, 1. , 1. ])
   >>> thresholds
   array([1.  , 0.8 , 0.4 , 0.35, 0.1 ])

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import roc_curve
   >>> target = [0, 1, 2]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.89, 0.06], [0.02, 0.03, 0.95]]
   >>> fpr, tpr, thresholds = roc_curve(target, preds, task='multiclass',
   ...     num_classes=3
   ... )
   >>> fpr
   [array([0. , 0. , 0.5, 1. ]),
   array([0. , 0. , 0.5, 1. ]),
   array([0. , 0. , 0.5, 1. ])]
   >>> tpr
   [array([0., 1., 1., 1.]), array([0., 1., 1., 1.]), array([0., 1., 1., 1.])]
   >>> thresholds
   [array([1.  , 0.9 , 0.05, 0.02]),
   array([1.  , 0.89, 0.05, 0.03]),
   array([1.  , 0.95, 0.06, 0.05])]

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import roc_curve
   >>> target = [[1, 1], [0, 1], [1, 0]]
   >>> preds = [[0.9, 0.8], [0.2, 0.7], [0.8, 0.3]]
   >>> fpr, tpr, thresholds = roc_curve(target, preds, task='multilabel',
   ...     num_labels=2
   ... )
   >>> fpr
   [array([0. , 0.5, 1. , 1. ]), array([0., 0., 0., 1.])]
   >>> tpr
   [array([0., 0., 0., 1.]), array([0. , 0.5, 1. , 1. ])]
   >>> thresholds
   [array([1. , 0.9, 0.8, 0.2]), array([1. , 0.8, 0.7, 0.3])]















   ..
       !! processed by numpydoc !!

.. py:function:: binary_sensitivity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn') -> float

   
   Compute sensitvity score for binary classification problems.

   Sensitivity is the recall of the positive class in a binary classification
   problem.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param pos_label: Label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: sensitivity score.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_sensitivity
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 0]
   >>> binary_sensitivity(target, preds)
   0.5















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_sensitivity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   
   Compute sensitivity score for multiclass classification problems.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param num_classes: Total number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the sensitivity will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: Optional[int]
   :param average: Average to apply. If None, return scores for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and false negatives.
                   - ``macro``: Calculate metrics for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their
                       average weighted by support (the number of true instances for each label).
                       This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there are no true positives or true negatives.
                         If set to ``warn``, this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: Sensitivity score. If ``average`` is None, return a numpy.ndarray of
             sensitivity scores for each class.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``average`` is not one of ``micro``, ``macro``, ``weighted``
       or ``None``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_sensitivity
   >>> target = [0, 1, 2, 0, 1, 2]
   >>> preds = [[0.4, 0.1, 0.5], [0.1, 0.8, 0.1], [0.2, 0.2, 0.6],
   ...     [0.5, 0.3, 0.2], [0.2, 0.5, 0.3], [0.2, 0.2, 0.6]]
   >>> multiclass_sensitivity(target, preds, num_classes=3, average="macro")
   0.8333333333333334















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_sensitivity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   
   Compute sensitivity score for multilabel classification tasks.

   The input is expected to be an array-like of shape (N, L), where N is the
   number of samples and L is the number of labels. The input is expected to
   be a binary array-like, where 1 indicates the presence of a label and 0
   indicates its absence.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param num_labels: Number of labels in the dataset.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param average: If ``None``, return the sensitivity score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: Sensitivity score. If ``average`` is None, return a numpy.ndarray of
             sensitivity scores for each label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``average`` is not one of ``micro``, ``macro``, ``weighted``
       or ``None``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_sensitivity
   >>> target = [1, 1, 2, 0, 2, 2]
   >>> preds = [1, 2, 2, 0, 2, 0]
   >>> multilabel_sensitivity(target, preds, num_classes=3)
   array([1.        , 0.5       , 0.66666667])















   ..
       !! processed by numpydoc !!

.. py:function:: sensitivity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: int = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: int = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute sensitivity score for different classification tasks.

   Sensitivity is the ratio tp / (tp + fn) where tp is the number of true positives
   and fn the number of false negatives. The sensitivity is intuitively the ability
   of the classifier to find all the positive samples.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param task: Task type.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label of the positive class. Only used for binary classification.
   :type pos_label: int
   :param num_classes: Number of classes. Only used for multiclass classification.
   :type num_classes: Optional[int]
   :param threshold: Threshold for positive class predictions.
   :type threshold: float, default=0.5
   :param top_k: Number of highest probability or logits predictions to consider when
                 computing multiclass or multilabel metrics. Default is None.
   :type top_k: Optional[int]
   :param num_labels: Number of labels. Only used for multilabel classification.
   :type num_labels: Optional[int]
   :param average: Average to apply. If None, return scores for each class/label. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take class/label imbalance into account.
                   - ``weighted``: Calculate metrics for each class/label, and find
                       their average weighted by support (the number of true instances for each
                       class/label). This alters ``macro`` to account for class/label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there are no true positives or true negatives.
                         If set to ``warn``, this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: Sensitivity score. If ``average`` is not None or ``task`` is ``binary``,
             return a float. Otherwise, return a numpy.ndarray of sensitivity scores
             for each class/label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``task`` is not one of ``binary``, ``multiclass`` or ``multilabel``.

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import sensitivity
   >>> target = [0, 1, 1, 0, 1]
   >>> preds = [0.4, 0.2, 0.0, 0.6, 0.9]
   >>> sensitivity(target, preds, task="binary")
   0.3333333333333333

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import sensitivity
   >>> target = [1, 1, 2, 0, 2, 2]
   >>> preds = [1, 2, 2, 0, 2, 0]
   >>> sensitivity(target, preds, task="multiclass", num_classes=3)
   array([1.        , 0.5       , 0.66666667])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import sensitivity
   >>> target = [[1, 0, 1], [0, 1, 0]]
   >>> preds = [[0.4, 0.2, 0.0], [0.6, 0.9, 0.1]]
   >>> sensitivity(target, preds, task="multilabel", num_labels=3)
   array([0., 1., 0.])















   ..
       !! processed by numpydoc !!

.. py:function:: binary_specificity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn') -> float

   
   Compute specificity for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param pos_label: The label to use for the positive class.
   :type pos_label: int, default=1
   :param threshold: The threshold to use for converting the predictions to binary
                     values. Logits will be converted to probabilities using the sigmoid
                     function.
   :type threshold: float, default=0.5

   :returns: The specificity score.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_specificity
   >>> target = [0, 1, 1, 0, 1]
   >>> preds = [0.1, 0.9, 0.8, 0.5, 0.4]
   >>> binary_specificity(target, preds)
   0.5















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_specificity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute specificity for multiclass classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param num_classes: The number of classes in the dataset.
   :type num_classes: int
   :param top_k: Number of highest probability or logit score predictions considered
                 to find the correct label. Only works when ``preds`` contain
                 probabilities/logits.
   :type top_k: int, optional
   :param average: If None, return the specificity for each class, otherwise return the
                   average specificity. Average options are:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives, false negatives, false positives and true negatives.
                   - ``macro``: Calculate metrics for each class, and find their unweighted
                       mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metrics for each class, and find their
                       average, weighted by support (the number of true instances for each
                       label).
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The specificity score. If ``average`` is None, a numpy.ndarray of
             shape (``num_classes``,) is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_specificity
   >>> target = [0, 1, 2, 0, 1, 2]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]]
   >>> multiclass_specificity(target, preds, num_classes=3)
   array([1.  , 0.75, 1.  ])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_specificity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute specificity for multilabel classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param num_labels: The number of labels in the dataset.
   :type num_labels: int
   :param threshold: The threshold value for converting probability or logit scores to
                     binary. A sigmoid function is first applied to logits to convert them
                     to probabilities.
   :type threshold: float, default=0.5
   :param top_k: Number of highest probability or logit score predictions considered
                 to find the correct label. Only works when ``preds`` contains
                 probabilities/logits.
   :type top_k: int, optional
   :param average: If None, return the specificity for each class, otherwise return the
                   average specificity. Average options are:

                   - ``micro``: Calculate metrics globally by counting the total
                       true positives, false negatives, false positives and true
                       negatives.
                   - ``macro``: Calculate metrics for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their
                       average, weighted by support (the number of true instances for
                       each label).
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The specificity score. If ``average`` is None, a numpy.ndarray of
             shape (``num_labels``,) is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_specificity
   >>> target = [[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 0, 1], [1, 0, 0]]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05]]
   >>> multilabel_specificity(target, preds, num_labels=3)
   array([0.5, 0., 0.5])















   ..
       !! processed by numpydoc !!

.. py:function:: specificity(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: int = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: int = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute specificity score for different classification tasks.

   The specificity is the ratio of true negatives to the sum of true negatives and
   false positives. It is also the recall of the negative class.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets as returned by a classifier.
   :type preds: ArrayLike
   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives, false positives, false negatives and true negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: **score** -- The specificity score. If ``average`` is ``None`` and ``task`` is not
             ``binary``, a numpy.ndarray of shape (``num_classes`` or ``num_labels``,)
             is returned.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``task`` is not one of ``binary``, ``multiclass``, or ``multilabel``.

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import specificity
   >>> target = [0, 1, 1, 0, 1]
   >>> preds = [0.9, 0.05, 0.05, 0.35, 0.05]
   >>> specificity(target, preds, task="binary")
   0.5

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import specificity
   >>> target = [0, 1, 2, 0, 1]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05]]
   >>> specificity(target, preds, task="multiclass", num_classes=3)
   array([0.5, 0., 0.5])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import specificity
   >>> target = [[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 0, 1], [1, 0, 0]]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.2, 0.75],
   ...          [0.35, 0.5, 0.15], [0.05, 0.9, 0.05]]
   >>> specificity(target, preds, task="multilabel", num_labels=3)
   array([0.5, 0., 0.5])















   ..
       !! processed by numpydoc !!

.. py:function:: binary_stat_scores(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, pos_label: int = 1, threshold: float = 0.5) -> numpy.ndarray

   
   Compute the stat scores for binary inputs.

   :param target: Ground truth.
   :type target: ArrayLike
   :param preds: Predictions.
   :type preds: ArrayLike
   :param pos_label: The label to use for the positive class.
   :type pos_label: int, default=1
   :param threshold: The threshold to use for converting the predictions to binary
                     values. Logits will be converted to probabilities using the sigmoid
                     function.
   :type threshold: float, default=0.5

   :returns: The true positives, false positives, true negatives and false negatives
             and support in that order.
   :rtype: numpy.ndarray

   :raises ValueError: If the threshold is not in [0, 1] or if the pos_label is not 0 or 1.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_stat_scores
   >>> target = [0, 1, 1, 0]
   >>> preds = [0, 1, 0, 0]
   >>> binary_stat_scores(target, preds)
   array([1, 0, 2, 1, 2])















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_stat_scores(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, top_k: Optional[int] = None, classwise: Optional[bool] = True) -> numpy.ndarray

   
   Compute stat scores for multiclass targets.

   :param target: The ground truth values.
   :type target: ArrayLike
   :param preds: The predictions. If determined to be in continuous format, will be
                 converted to multiclass using the ``top_k`` parameter.
   :type preds: ArrayLike
   :param num_classes: The total number of classes for the problem.
   :type num_classes: int
   :param top_k: The number of top predictions to consider when computing the
                 stat scores. If ``None``, it is assumed to be 1.
   :type top_k: Optional[int], default=None
   :param classwise: Whether to return the stat scores for each class or sum over all
                     classes.
   :type classwise: bool, default=True

   :returns: The number of true positives, false positives, true negatives, false
             negatives and support. If ``classwise`` is ``True``, the shape is
             ``(num_classes, 5)``. Otherwise, the shape is ``(5,)``
   :rtype: numpy.nadarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_stat_scores
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 2, 1, 2, 0]
   >>> multiclass_stat_scores(target, preds, num_classes=3)
   array([[1, 1, 3, 0, 1],
           [0, 1, 3, 1, 1],
           [1, 1, 1, 2, 3]])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_stat_scores(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, labelwise: Optional[bool] = False) -> numpy.ndarray

   
   Compute the stat scores for multilabel inputs.

   :param target: Ground truth.
   :type target: ArrayLike
   :param preds: Predictions.
   :type preds: ArrayLike
   :param num_labels: The total number of labels for the problem.
   :type num_labels: int
   :param threshold: Threshold value for binarizing predictions that are probabilities or
                     logits. A sigmoid function is applied if the predictions are logits.
   :type threshold: float, default=0.5
   :param top_k: The number of top predictions to consider when computing the statistics.
   :type top_k: int, default=None
   :param labelwise: Whether to return the stat scores for each label or sum over all labels.
   :type labelwise: bool, default=False

   :returns: The number of true positives, false positives, true negatives and false
             negatives and the support. The shape of the array is ``(5, num_labels)``
             if ``labelwise=True`` and ``(5,)`` otherwise.
   :rtype: numpy.ndarray

   :raises ValueError: If ``threshold`` is not between ``0`` and ``1``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_stat_scores
   >>> target = [[0, 1, 1], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.8, 0.2, 0.7]]
   >>> multilabel_stat_scores(target, preds, num_labels=3)
   array([[1, 0, 1, 0, 1],
           [1, 0, 1, 0, 1],
           [2, 0, 0, 0, 2]])















   ..
       !! processed by numpydoc !!

.. py:function:: stat_scores(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, threshold: float = 0.5, num_classes: Optional[int] = None, classwise: Optional[bool] = True, top_k: Optional[int] = None, num_labels: Optional[int] = None, labelwise: Optional[bool] = False) -> numpy.ndarray

   
   Compute stat scores for binary, multiclass or multilabel problems.

   This function acts as an entry point to the specialized functions for each
   task.

   :param target: Ground truth.
   :type target: ArrayLike
   :param preds: Predictions.
   :type preds: ArrayLike
   :param task: The task type. Can be either ``binary``, ``multiclass`` or
                ``multilabel``.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: The positive label to report. Only used for binary tasks.
   :type pos_label: int, default=1
   :param threshold: The threshold to use for binarizing the predictions if logits or
                     probabilities are provided. If logits are provided, a sigmoid function
                     is applied prior to binarization. Used for binary and multilabel tasks.
   :type threshold: float, default=0.5
   :param num_classes: The number of classes for the problem. Required for multiclass tasks.
   :type num_classes: int
   :param classwise: Whether to return the stat scores for each class or sum over all
                     classes. Only used for multiclass tasks.
   :type classwise: bool, default=True
   :param top_k: The number of top predictions to consider when computing the statistics.
                 If ``None``, ``top_k`` is set to 1. Used for multiclass and multilabel
                 tasks.
   :type top_k: int, default=None
   :param num_labels: The number of labels. Only used for multilabel tasks.
   :type num_labels: int
   :param labelwise: Whether to compute the stat scores labelwise. Only used for multilabel
                     tasks.
   :type labelwise: bool, default=False

   :returns: **scores** -- The stat scores - true positives, false positives, true negatives,
             false negatives and support. For binary tasks, the shape is (5,).
             For multiclass tasks, the shape is (n_classes, 5) if ``classwise`` is
             True, otherwise (5,). For multilabel tasks, the shape is (n_labels, 5)
             if ``labelwise`` is True, otherwise (n_classes, 5).
   :rtype: numpy.ndarray

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import tat_scores
   >>> target = [0, 1, 1, 0]
   >>> preds = [0, 1, 0, 0]
   >>> stat_scores(target, preds, task="binary")
   array([1, 0, 2, 1, 2])

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import multiclass_stat_scores
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 2, 1, 2, 0]
   >>> stat_scores(target, preds, task="multiclass", num_classes=3)
   array([[1, 1, 3, 0, 1],
           [0, 1, 3, 1, 1],
           [1, 1, 1, 2, 3]])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import stat_scores
   >>> target = [[0, 1, 1], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.8, 0.2, 0.7]]
   >>> stat_scores(target, preds, task="multilabel", num_labels=3)
   array([[1, 0, 1, 0, 1],
           [1, 0, 1, 0, 1],
           [2, 0, 0, 0, 2]])















   ..
       !! processed by numpydoc !!

