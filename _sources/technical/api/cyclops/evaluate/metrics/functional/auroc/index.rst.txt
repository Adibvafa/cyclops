:py:mod:`cyclops.evaluate.metrics.functional.auroc`
===================================================

.. py:module:: cyclops.evaluate.metrics.functional.auroc

.. autoapi-nested-parse::

   Functions for computing the area under the ROC curve (AUROC).

   ..
       !! processed by numpydoc !!


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.functional.auroc._reduce_auroc
   cyclops.evaluate.metrics.functional.auroc._binary_auroc_compute
   cyclops.evaluate.metrics.functional.auroc.binary_auroc
   cyclops.evaluate.metrics.functional.auroc._multiclass_auroc_compute
   cyclops.evaluate.metrics.functional.auroc.multiclass_auroc
   cyclops.evaluate.metrics.functional.auroc._multilabel_auroc_compute
   cyclops.evaluate.metrics.functional.auroc.multilabel_auroc
   cyclops.evaluate.metrics.functional.auroc.auroc



.. py:function:: _reduce_auroc(fpr: Union[numpy.ndarray, List[numpy.ndarray]], tpr: Union[numpy.ndarray, List[numpy.ndarray]], average: Literal[macro, weighted] = None, weights: numpy.ndarray = None) -> Union[float, numpy.ndarray]

   
   Compute the area under the ROC curve and apply ``average`` method.

   :param fpr: False positive rate.
   :type fpr: numpy.ndarray or list of numpy.ndarray
   :param tpr: True positive rate.
   :type tpr: numpy.ndarray or list of numpy.ndarray
   :param average: If not None, apply the method to compute the average area under the
                   ROC curve.
   :type average: Literal["macro", "weighted"], default=None
   :param weights: Sample weights.
   :type weights: numpy.ndarray, default=None

   :returns: **auroc** -- Area under the ROC curve. If ``average`` is not None, ``auroc`` is a
             numpy array.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``average`` is not one of ``macro`` or ``weighted`` or if
       ``average`` is ``weighted`` and ``weights`` is None.















   ..
       !! processed by numpydoc !!

.. py:function:: _binary_auroc_compute(state: Union[Tuple[numpy.ndarray, numpy.ndarray], numpy.ndarray], thresholds: numpy.ndarray = None, max_fpr: float = None, pos_label: int = 1) -> float

   
   Compute the area under the ROC curve for binary classification tasks.

   :param state: If ``state`` is a tuple, then it must be a tuple of two numpy arrays
                 ``(target, preds)``. If ``state`` is a numpy array, then it is a multi-
                 threshold confusion matrix.
   :type state: Union[Tuple[numpy.ndarray, numpy.ndarray], numpy.ndarray]
   :param thresholds: Thresholds used for computing binarizing the predictions. If None,
                      then the thresholds are automatically determined by the unique values
                      in ``preds``.
   :type thresholds: numpy.ndarray, default=None
   :param max_fpr: The maximum value of the false positive rate. If ``None``, the
                   false positive rate is set to the complement of the true positive
                   rate.
   :type max_fpr: float, default=None
   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1

   :returns: **auroc** -- Area under the ROC curve.
   :rtype: float















   ..
       !! processed by numpydoc !!

.. py:function:: binary_auroc(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, max_fpr: float = None, thresholds: Union[int, List[float], numpy.ndarray] = None, pos_label: int = 1) -> float

   
   Compute the area under the ROC curve for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If the values in ``preds``
                 are not in the range [0, 1], then they will be transformed to this range
                 via a sigmoid function.
   :type preds: ArrayLike
   :param max_fpr: The maximum value of the false positive rate. If not None, then
                   the partial AUCROC in the range [0, max_fpr] is returned.
   :type max_fpr: float, default=None
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None
   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1

   :returns: **auroc** -- Area under the ROC curve.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_auroc
   >>> target = [1, 0, 0, 1]
   >>> preds = [0.1, 0.9, 0.4, 0.6]
   >>> binary_auroc(target, preds, thresholds=5)
   0.25















   ..
       !! processed by numpydoc !!

.. py:function:: _multiclass_auroc_compute(state: Union[numpy.ndarray, Tuple[numpy.ndarray, numpy.ndarray]], num_classes: int, thresholds: numpy.ndarray = None, average: Literal[macro, weighted] = None) -> Union[float, numpy.ndarray]

   
   Compute the area under the ROC curve for multiclass classification tasks.

   :param state: If ``state`` is a numpy array, then it is a multi-threshold confusion
                 matrix. If ``state`` is a tuple, then it must be a tuple of two numpy
                 arrays ``(target, preds)``.
   :type state: Union[numpy.ndarray, Tuple[numpy.ndarray, numpy.ndarray]]
   :param num_classes: Number of classes.
   :type num_classes: int
   :param thresholds: Thresholds used for computing binarizing the predictions. If None,
                      then the thresholds are automatically determined by the unique values
                      in ``preds``.
   :type thresholds: numpy.ndarray, default=None
   :param average: If ``None``, then the scores for each class are returned. Otherwise,
                   this determines the type of averaging performed on the scores.
   :type average: Literal["macro", "weighted"], default=None

   :returns: **auroc** -- Area under the ROC curve. If ``average`` is ``None``, then a numpy array
             of shape (num_classes,) is returned, otherwise a float is returned.
   :rtype: Union[float, numpy.ndarray]















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_auroc(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, thresholds: Union[int, List[float], numpy.ndarray] = None, average: Literal[macro, weighted] = None) -> Union[float, numpy.ndarray]

   
   Compute the area under the ROC curve for multiclass classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If the values in ``preds``
                 are not in the range [0, 1], then they will be transformed to this range
                 via a softmax function.
   :type preds: ArrayLike
   :param num_classes: Number of classes.
   :type num_classes: int
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None
   :param average: If ``None``, then the scores for each class are returned. Otherwise,
                   this determines the type of averaging performed on the scores. One of

                   - `macro`: Calculate metrics for each class, and find their unweighted
                       mean. This does not take class imbalance into account.
                   - `weighted`: Calculate metrics for each class, and find their average,
                       weighted by support (the number of true instances for each class).
   :type average: Literal["macro", "weighted"], default=None

   :returns: **auroc** -- Area under the ROC curve. If ``average`` is ``None``, then a numpy array
             of shape (num_classes,) is returned, otherwise a float is returned.
   :rtype: Union[float, numpy.ndarray]

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_auroc
   >>> target = [1, 0, 2, 0]
   >>> preds = [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05],
   ...         [0.05, 0.05, 0.9], [0.9, 0.05, 0.05]]
   >>> multiclass_auroc(target, preds, num_classes=3, thresholds=5,
   ...     average=None
   ... )
   array([0.5       , 0.33333333, 1.        ])















   ..
       !! processed by numpydoc !!

.. py:function:: _multilabel_auroc_compute(state: Union[Tuple[numpy.ndarray, numpy.ndarray], numpy.ndarray], num_labels: int, thresholds: numpy.ndarray = None, average: Literal[micro, macro, weighted] = None) -> Union[float, numpy.ndarray]

   
   Compute the area under the ROC curve for multilabel classification tasks.

   :param state: If ``state`` is a numpy array, then it is a multi-threshold confusion
                 matrix. If ``state`` is a tuple, then it must be a tuple of two numpy
                 arrays ``(target, preds)``.
   :type state: Union[Tuple[numpy.ndarray, numpy.ndarray], numpy.ndarray]
   :param num_labels: Number of labels.
   :type num_labels: int
   :param thresholds: Thresholds used for computing binarizing the predictions. If None,
                      then the thresholds are automatically determined by the unique values
                      in ``preds``.
   :type thresholds: numpy.ndarray, default=None
   :param average: If ``None``, then the scores for each label are returned. Otherwise,
                   this determines the type of averaging performed on the scores. One of

                   - `micro`: Calculate metrics globally.
                   - `macro`: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - `weighted`: Calculate metrics for each label, and find their average,
                       weighted by support (the number of true instances for each label).
   :type average: Literal["micro", "macro", "weighted"], default=None

   :returns: Area under the ROC curve. If ``average`` is ``None``, then a numpy array
             of shape (num_labels,) is returned, otherwise a float is returned.
   :rtype: float or numpy.ndarray















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_auroc(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, thresholds: Union[int, List[float], numpy.ndarray] = None, average: Literal[micro, macro, weighted] = None) -> Union[float, numpy.ndarray]

   
   Compute the area under the ROC curve for multilabel classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated probabilities or decision function. If the values in ``preds``
                 are not in the range [0, 1], then they will be transformed to this range
                 via a softmax function.
   :type preds: ArrayLike
   :param num_labels: Number of labels.
   :type num_labels: int
   :param thresholds: Thresholds used for binarizing the values of ``preds``.
                      If int, then the number of thresholds to use.
                      If list or array, then the thresholds to use.
                      If None, then the thresholds are automatically determined by the
                      unique values in ``preds``.
   :type thresholds: Union[int, List[float], numpy.ndarray], default=None
   :param average: If ``None``, then the scores for each label are returned. Otherwise,
                   this determines the type of averaging performed on the scores. One of

                   - `micro`: Calculate metrics globally by counting the total true
                       positives, false negatives and false positives.
                   - `macro`: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - `weighted``: Calculate metrics for each label, and find their average,
                       weighted by support (the number of true instances for each label).
   :type average: Literal["micro", "macro", "weighted"], default=None

   :returns: Area under the ROC curve. If ``average`` is ``None``, then a numpy array
             of shape (num_labels,) is returned, otherwise a float is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_auroc
   >>> target = [[0, 1, 0], [0, 1, 1], [1, 0, 1]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.9], [0.8, 0.2, 0.3]]
   >>> multilabel_auroc(target, preds, num_labels=3, thresholds=5,
   ...     average=None)
   array([1.  , 0.75, 0.25])















   ..
       !! processed by numpydoc !!

.. py:function:: auroc(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], max_fpr: float = None, thresholds: Union[int, List[float], numpy.ndarray] = None, num_classes: int = None, num_labels: int = None, average: Literal[micro, macro, weighted] = None) -> Union[float, numpy.ndarray]

   
   Compute the area under the ROC curve for different tasks.

   target : ArrayLike
       Ground truth (correct) target values.
   preds : ArrayLike
       Estimated probabilities or decision function. If ``preds`` is not in the
       range [0, 1], a sigmoid function is applied to transform it to the range
       [0, 1].
   task : Literal["binary", "multiclass", "multilabel"]
       Task type. One of ``binary``, ``multiclass``, ``multilabel``.
   max_fpr : float, default=None
       The maximum value of the false positive rate. If not None, a partial AUC
       in the range [0, max_fpr] is returned. Only used for binary classification.
   thresholds : int or list of floats or numpy.ndarray of floats, default=None
       Thresholds used for binarizing the values of ``preds``.
       If int, then the number of thresholds to use.
       If list or array, then the thresholds to use.
       If None, then the thresholds are automatically determined by the unique
       values in ``preds``.
   num_classes : int, default=None
       Number of classes. This parameter is required for the ``multiclass`` task.
   num_labels : int, default=None
       Number of labels. This parameter is required for the ``multilabel`` task.
   average : Literal["micro", "macro", "weighted"], default=None
       If not None, apply the method to compute the average area under the
       ROC curve. Only applicable for the ``multiclass`` and ``multilabel``
       tasks. One of:

       - ``micro``: Calculate metrics globally by counting the total true
           positives, false negatives and false positives.
       - ``macro``: Calculate metrics for each label, and find their unweighted
           mean. This does not take label imbalance into account.
       - ``weighted``: Calculate metrics for each label, and find their
           average, weighted by support (accounting for label imbalance).

   :returns: **auroc_score** -- Area under the ROC curve. If ``average`` is None or task is ``binary``,
             ``auroc`` is a float. Otherwise, ``auroc`` is a numpy array.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import auroc
   >>> target = [0, 1, 0, 1]
   >>> preds = [0.1, 0.35, 0.4, 0.8]
   >>> auroc(target, preds, task="binary")
   0.75

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import auroc
   >>> target = [0, 1, 2, 0, 1, 2]
   >>> preds = [[0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.5, 0.3, 0.2],
   ...          [0.1, 0.6, 0.3], [0.05, 0.95, 0], [0.5, 0.3, 0.2]]
   >>> auroc(target, preds, task="multiclass", num_classes=3, average=None)
   array([0.5, 1. , 0.5])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import auroc
   >>> target = [[0, 1], [1, 1], [0, 0], [1, 0]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2], [0.4, 0.6], [0.2, 0.8]]
   >>> auroc(target, preds, task="multilabel", num_labels=2, average=None)
   array([0.25, 0.5 ])















   ..
       !! processed by numpydoc !!

