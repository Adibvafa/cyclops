:py:mod:`cyclops.evaluate.metrics.functional.f_beta`
====================================================

.. py:module:: cyclops.evaluate.metrics.functional.f_beta

.. autoapi-nested-parse::

   Functions for computing F-beta and F1 scores for different input types.

   ..
       !! processed by numpydoc !!


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.functional.f_beta._fbeta_reduce
   cyclops.evaluate.metrics.functional.f_beta._check_beta
   cyclops.evaluate.metrics.functional.f_beta.binary_fbeta_score
   cyclops.evaluate.metrics.functional.f_beta.multiclass_fbeta_score
   cyclops.evaluate.metrics.functional.f_beta.multilabel_fbeta_score
   cyclops.evaluate.metrics.functional.f_beta.fbeta_score
   cyclops.evaluate.metrics.functional.f_beta.binary_f1_score
   cyclops.evaluate.metrics.functional.f_beta.multiclass_f1_score
   cyclops.evaluate.metrics.functional.f_beta.multilabel_f1_score
   cyclops.evaluate.metrics.functional.f_beta.f1_score



.. py:function:: _fbeta_reduce(tp: numpy.ndarray, fp: numpy.ndarray, fn: numpy.ndarray, beta: float, average: Literal[micro, macro, weighted, None], zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the F-beta score, a generalization of F-measure.

   :param tp: True positives per class.
   :type tp: numpy.ndarray
   :param fp: False positives per class.
   :type fp: numpy.ndarray
   :param fn: False negatives per class.
   :type fn: numpy.ndarray
   :param beta: Weight of precision in harmonic mean (beta < 1 lends more weight to
                precision, beta > 1 favors recall).
   :type beta: float
   :param average: If ``None``, return the score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label/class imbalance
                       into account.
                   - ``weighted``: Calculate metric for each label/class, and find their
                       average weighted by the support (the number of true instances
                       for each label/class). This alters "macro" to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: **result** -- F-beta score or array of scores if ``average=None``.
   :rtype: float or numpy.ndarray

   :raises ValueError: if beta is less than 0.















   ..
       !! processed by numpydoc !!

.. py:function:: _check_beta(beta: float) -> None

   
   Check the ``beta`` argument for F-beta metrics.
















   ..
       !! processed by numpydoc !!

.. py:function:: binary_fbeta_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, beta: float, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn') -> float

   
   Compute the F-beta score for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param pos_label: The positive class label. One of [0, 1].
   :type pos_label: int, default=1
   :param threshold: Threshold value for converting probabilities and logits to binary.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there are no true positives or true negatives.
                         If set to ``warn``, this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The binary F-beta score.
   :rtype: float

   :raises ValueError: beta is less than 0.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_fbeta_score
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> binary_fbeta_score(target, preds, beta=0.5)
   0.7142857142857143















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_fbeta_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, beta: float, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the F-beta score for multiclass data.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param num_classes: The number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The multiclass F-beta score. If ``average`` is ``None``, a numpy array
             of shape (num_classes,) is returned.
   :rtype: float or numpy.ndarray

   :raises ValueError: ``average`` is not one of ``micro``, ``macro``, ``weighted``, or ``None``,
       or ``beta`` is less than 0.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_fbeta_score
   >>> target = [0, 1, 2, 0]
   >>> preds = [0, 2, 1, 0]
   >>> multiclass_fbeta_score(target, preds, beta=0.5, num_classes=3)
   array([1., 0., 0.])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_fbeta_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, beta: float, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   
   Compute the F-beta score for multilabel data.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the score will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each label. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label
                       imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The multilabel F-beta score. If ``average`` is ``None``, a numpy array
             of shape (num_labels,) is returned.
   :rtype: float or numpy.ndarray

   :raises ValueError: ``average`` is not one of ``micro``, ``macro``, ``weighted``, or ``None``,
       or ``beta`` is less than 0.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_fbeta_score
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> multilabel_fbeta_score(target, preds, beta=0.5, num_labels=2)
   array([1.        , 0.83333333])















   ..
       !! processed by numpydoc !!

.. py:function:: fbeta_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, beta: float, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: int = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: int = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the F-beta score for binary, multiclass, or multilabel data.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets as returned by a classifier.
   :type preds: ArrayLike
   :param beta: Weight of precision in harmonic mean.
   :type beta: float
   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: **score** -- The F-beta score. If ``average`` is not ``None`` and ``task`` is not
             ``binary``, a numpy array of shape (num_classes,) is returned.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``task`` is not one of ``binary``, ``multiclass``, or
       ``multilabel``.

   .. rubric:: Examples

   (binary)
   >>> from cyclops.evaluation.metrics.functional import fbeta_score
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.8, 0.4, 0.3]
   >>> fbeta_score(target, preds, beta=0.5, task="binary")
   0.8333333333333334

   (multiclass)
   >>> from cyclops.evaluation.metrics.functional import fbeta_score
   >>> target = [0, 1, 2, 2]
   >>> preds = [1 2, 2, 0]
   >>> fbeta_score(target, preds, beta=0.5, task="multiclass", num_classes=3)
   array([0.83333333, 0.        , 0.55555556])

   (multilabel)
   >>> from cyclops.evaluation.metrics.functional import fbeta_score
   >>> target = [[0, 1], [1, 1]]
   >>> preds = [[0.1, 0.9], [0.8, 0.2]]
   >>> fbeta_score(target, preds, beta=0.5, task="multilabel", num_labels=2)
   array([1.        , 0.83333333])















   ..
       !! processed by numpydoc !!

.. py:function:: binary_f1_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn') -> float

   
   Compute the F1 score for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param pos_label: The label of the positive class.
   :type pos_label: int, default=1
   :param threshold: Threshold value for binarizing predictions in form of logits or
                     probability scores.
   :type threshold: float, default=0.5
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The F1 score.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_f1_score
   >>> target = [0, 1, 1, 0]
   >>> preds = [0.1, 0.8, 0.4, 0.3]
   >>> binary_f1_score(target, preds)
   0.6666666666666666















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_f1_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the F1 score for multiclass classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each class. Otherwise, use one of
                   the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metric for each class, and find their
                       unweighted mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metric for each class, and find their
                       average weighted by the support (the number of true instances
                       for each class). This alters "macro" to account for class
                       imbalance. It can result in an F-score that is not between
                       precision and recall.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The F1 score. If ``average`` is ``None``, a numpy.ndarray of shape
             (``num_classes``,) is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_f1_score
   >>> target = [0, 1, 2, 0]
   >>> preds = [1, 1, 1, 0]
   >>> multiclass_f1_score(target, preds, num_classes=3)
   array([0.66666667, 0.5       , 0.        ])















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_f1_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn')

   
   Compute the F1 score for multilabel classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Predictions as returned by a classifier.
   :type preds: ArrayLike
   :param num_labels: Number of labels for the task.
   :type num_labels: int
   :param threshold: Threshold for deciding the positive class.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1.
   :type top_k: int, optional
   :param average: If ``None``, return the score for each label. Otherwise, use one of
                   the following options to compute the average score:

                   - ``micro``: Calculate metric globally from the total count of true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metric for each label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metric for each label, and find their
                       average weighted by the support (the number of true instances
                       for each label). This alters "macro" to account for label imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The F1 score. If ``average`` is ``None``, a numpy.ndarray of shape
             (``num_labels``,) is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_f1_score
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.2]]
   >>> multilabel_f1_score(target, preds, num_labels=3)
   array([0., 1., 1.])















   ..
       !! processed by numpydoc !!

.. py:function:: f1_score(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: int = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: int = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the F1 score for multiclass data.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets as returned by a classifier.
   :type preds: ArrayLike
   :param task: Type of classification task.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives, false positives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label/class imbalance into
                       account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Value to return when there is a zero division. If set to "warn", this
                         acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The F1 score. If ``average`` is ``None`` and ``task`` is not ``binary``,
             a numpy.ndarray of shape (``num_classes`` or ``num_labels``,) is returned.
   :rtype: float or numpy.ndarray

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import f1_score
   >>> target = [0, 1, 0, 1]
   >>> preds = [0.1, 0.9, 0.8, 0.2]
   >>> f1_score(target, preds, task="binary")
   0.5

   >>> # (multiclass)
   >>> from cyclops.evaluation.metrics.functional import f1_score
   >>> target = [0, 1, 2, 0]
   >>> preds = [[0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.2, 0.2, 0.6], [0.9, 0.1, 0]]
   >>> f1_score(target, preds, task="multiclass", num_classes=3)
   array([0.66666667, 0.8       , 0.        ])

   >>> # (multilabel)
   >>> from cyclops.evaluation.metrics.functional import f1_score
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0.1, 0.9, 0.8], [0.05, 0.1, 0.2]]
   >>> f1_score(target, preds, task="multilabel", num_labels=3)
   array([0., 1., 1.])















   ..
       !! processed by numpydoc !!

