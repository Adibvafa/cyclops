:py:mod:`cyclops.evaluate.metrics.functional.accuracy`
======================================================

.. py:module:: cyclops.evaluate.metrics.functional.accuracy

.. autoapi-nested-parse::

   Functions for computing accuracy scores for classification tasks.

   ..
       !! processed by numpydoc !!


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   cyclops.evaluate.metrics.functional.accuracy._accuracy_reduce
   cyclops.evaluate.metrics.functional.accuracy.binary_accuracy
   cyclops.evaluate.metrics.functional.accuracy.multiclass_accuracy
   cyclops.evaluate.metrics.functional.accuracy.multilabel_accuracy
   cyclops.evaluate.metrics.functional.accuracy.accuracy



.. py:function:: _accuracy_reduce(tp: Union[numpy.ndarray, numpy.int_], fp: Union[numpy.ndarray, numpy.int_], tn: Union[numpy.ndarray, numpy.int_], fn: Union[numpy.ndarray, numpy.int_], task_type: Literal[binary, multiclass, multilabel], average: Literal[micro, macro, weighted, None], zero_division: Literal[warn, 0, 1] = 'warn') -> Union[numpy.ndarray, float]

   
   Compute accuracy score per class or sample and apply average.

   :param tp: The number of true positives.
   :type tp: numpy.ndarray or int
   :param fp: The number of false positives.
   :type fp: numpy.ndarray or int
   :param tn: The number of true negatives.
   :type tn: numpy.ndarray or int
   :param fn: The number of false negatives.
   :type fn: numpy.ndarray or int
   :param task_type: The type of task for the input data. One of 'binary', 'multiclass'
                     or 'multilabel'.
   :type task_type: Literal["binary", "multiclass", "multilabel"]
   :param average: The type of averaging to apply to the accuracy scores. One of
                   'micro', 'macro', 'weighted' or None.
   :type average: Literal["micro", "macro", "weighted", None]
   :param zero_division: Sets the value to return when there is a zero division. If set to "warn",
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1]

   :returns: **accuracy** -- The average accuracy score if 'average' is not None, otherwise the
             accuracy score per class.
   :rtype: numpy.ndarray or float















   ..
       !! processed by numpydoc !!

.. py:function:: binary_accuracy(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, pos_label: int = 1, threshold: float = 0.5, zero_division: Literal[warn, 0, 1] = 'warn') -> float

   
   Compute accuracy score for binary classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param pos_label: The label of the positive class. Can be 0 or 1.
   :type pos_label: int, default=1
   :param threshold: The threshold value for converting probability or logit scores to
                     binary. A sigmoid function is first applied to logits to convert them
                     to probabilities.
   :type threshold: float, default=0.5
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The accuracy score.
   :rtype: float

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import binary_accuracy
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> binary_accuracy(target, preds)
   0.75
   >>> target = [0, 1, 0, 1]
   >>> preds = [0.1, 0.9, 0.8, 0.4]
   >>> binary_accuracy(target, preds, threshold=0.5)
   0.5















   ..
       !! processed by numpydoc !!

.. py:function:: multiclass_accuracy(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_classes: int, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the accuracy score for multiclass classification problems.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param num_classes: Number of classes in the dataset.
   :type num_classes: int
   :param top_k: Number of highest probability predictions or logits to consider when
                 computing the accuracy score.
   :type top_k: int, default=None
   :param average: If not None, this determines the type of averaging performed on the data:

                   - ``micro``: Calculate metrics globally by counting the total
                       true positives, false negatives, false positives and true
                       negatives.
                   - ``macro``: Calculate metrics for each class, and find their
                       unweighted mean. This does not take class imbalance into account.
                   - ``weighted``: Calculate metrics for each class, and find their
                       average, weighted by support (the number of true instances for
                       each class). This alters ``macro`` to account for class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: The average accuracy score as a float if ``average`` is not None,
             otherwise a numpy array of accuracy scores per class/label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``average`` is not one of ``micro``, ``macro``, ``weighted`` or ``None``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multiclass_accuracy
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 0, 2, 2, 1]
   >>> multiclass_accuracy(target, preds, num_classes=3)
   array([1.        , 0.        , 0.66666667])
   >>> multiclass_accuracy(target, preds, num_classes=3, average="micro")
   0.6
   >>> multiclass_accuracy(target, preds, num_classes=3, average="macro")
   0.5555555555555555
   >>> multiclass_accuracy(target, preds, num_classes=3, average="weighted")
   0.6















   ..
       !! processed by numpydoc !!

.. py:function:: multilabel_accuracy(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, num_labels: int, threshold: float = 0.5, top_k: Optional[int] = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute the accuracy score for multilabel-indicator targets.

   :param target: Ground truth (correct) target values.
   :type target: array-like of shape (num_samples, num_labels)
   :param preds: Estimated targets as returned by a classifier.
   :type preds: array-like of shape (num_samples, num_labels)
   :param num_labels: Number of labels in the multilabel classification task.
   :type num_labels: int
   :param threshold: Threshold value for binarizing the output of the classifier.
   :type threshold: float, default=0.5
   :param top_k: The number of highest probability or logit predictions considered
                 to find the correct label. Only works when ``preds`` contains
                 probabilities/logits.
   :type top_k: int, optional, default=None
   :param average: If None, return the accuracy score per label, otherwise this determines
                   the type of averaging performed on the data:

                   - ``micro``: Calculate metrics globally by counting the total
                       true positives, false negatives, true negatives and false positives.
                   - ``macro``: Calculate metrics for each label, and find their unweighted
                       mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label, and find their
                       average, weighted by support (the number of true instances for
                       each label).
   :type average: Literal['micro', 'macro', 'weighted', None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal['warn', 0, 1], default="warn"

   :returns: The average accuracy score as a flot if ``average`` is not None,
             otherwise a numpy array of accuracy scores per label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``average`` is not one of ``micro``, ``macro``, ``weighted``,
       or ``None``.

   .. rubric:: Examples

   >>> from cyclops.evaluation.metrics.functional import multilabel_accuracy
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0, 1, 0], [1, 0, 1]]
   >>> multilabel_accuracy(target, preds, num_labels=3, average=None)
   array([1., 1., 0.])
   >>> multilabel_accuracy(target, preds, num_labels=3, average="micro")
   0.6666666666666666















   ..
       !! processed by numpydoc !!

.. py:function:: accuracy(target: numpy.typing.ArrayLike, preds: numpy.typing.ArrayLike, task: Literal[binary, multiclass, multilabel], pos_label: int = 1, num_classes: int = None, threshold: float = 0.5, top_k: Optional[int] = None, num_labels: int = None, average: Literal[micro, macro, weighted, None] = None, zero_division: Literal[warn, 0, 1] = 'warn') -> Union[float, numpy.ndarray]

   
   Compute accuracy score for different classification tasks.

   :param target: Ground truth (correct) target values.
   :type target: ArrayLike
   :param preds: Estimated targets (predictions) as returned by a classifier.
   :type preds: ArrayLike
   :param task: The type of task for the input data. One of 'binary', 'multiclass'
                or 'multilabel'.
   :type task: Literal["binary", "multiclass", "multilabel"]
   :param pos_label: Label to consider as positive for binary classification tasks.
   :type pos_label: int, default=1
   :param num_classes: Number of classes for the task. Required if ``task`` is ``"multiclass"``.
   :type num_classes: int, default=None
   :param threshold: Threshold for deciding the positive class. Only used if ``task`` is
                     ``"binary"`` or ``"multilabel"``.
   :type threshold: float, default=0.5
   :param top_k: If given, and predictions are probabilities/logits, the precision will
                 be computed only for the top k classes. Otherwise, ``top_k`` will be
                 set to 1. Only used if ``task`` is ``"multiclass"`` or ``"multilabel"``.
   :type top_k: int, optional
   :param num_labels: Number of labels for the task. Required if ``task`` is ``"multilabel"``.
   :type num_labels: int, default=None
   :param average: If ``None``, return the recall score for each label/class. Otherwise,
                   use one of the following options to compute the average score:

                   - ``micro``: Calculate metrics globally by counting the total true
                       positives. false positives, true negatives and false negatives.
                   - ``macro``: Calculate metrics for each class/label, and find their
                       unweighted mean. This does not take label imbalance into account.
                   - ``weighted``: Calculate metrics for each label/class, and find
                       their average weighted by support (the number of true instances
                       for each label/class). This alters ``macro`` to account for
                       label/class imbalance.
   :type average: Literal["micro", "macro", "weighted", None], default=None
   :param zero_division: Sets the value to return when there is a zero division. If set to ``warn``,
                         this acts as 0, but warnings are also raised.
   :type zero_division: Literal["warn", 0, 1], default="warn"

   :returns: **accuracy_score** -- The average accuracy score as a float if ``average`` is not None,
             otherwise a numpy array of accuracy scores per class/label.
   :rtype: float or numpy.ndarray

   :raises ValueError: If ``task`` is not one of ``binary``, ``multiclass`` or ``multilabel``.
   :raises AssertionError: If ``task`` is ``multiclass`` and ``num_classes`` is not provided or is
       less than 0.
   :raises AssertionError: If ``task`` is ``multilabel`` and ``num_labels`` is not provided or is
       less than 0.

   .. rubric:: Examples

   >>> # (binary)
   >>> from cyclops.evaluation.metrics.functional import accuracy
   >>> target = [0, 1, 0, 1]
   >>> preds = [0, 1, 1, 1]
   >>> accuracy(target, preds, task="binary")
   0.75

   >>> # (multiclass)
   >>> target = [0, 1, 2, 2, 2]
   >>> preds = [0, 0, 2, 2, 1]
   >>> accuracy(target, preds, task="multiclass", num_classes=3, average="micro")
   0.6

   >>> # (multilabel)
   >>> target = [[0, 1, 1], [1, 0, 0]]
   >>> preds = [[0, 1, 0], [1, 0, 1]]
   >>> accuracy(target, preds, task="multilabel", num_labels=3, average="mcro")
   0.6666666666666666















   ..
       !! processed by numpydoc !!

